---
---

@string{aps = {American Physical Society,}}

@article{huangDiscriminativeRadialDomain2023,
  bibtex_show={true},
  abbr={IEEE TIP},
  ids = {huangDiscriminativeRadialDomain2023a},
  title = {Discriminative {{Radial Domain Adaptation}}},
  author = {Huang, Zenan and Wen, Jun and Chen, Siheng and Zhu, Linchao and Zheng, Nenggan},
  year = {2023},
  journal = {IEEE Transactions on Image Processing},
  pages = {1--1},
  issn = {1941-0042},
  doi = {10.1109/TIP.2023.3235583},
  copyright = {All rights reserved},
  html={/projects/001_drda},
  abstract={Domain adaptation methods reduce domain shift typically by learning domain-invariant features. Most existing methods are built on distribution matching, e.g., adversarial domain adaptation, which tends to corrupt feature discriminability. In this paper, we propose Discriminative Radial Domain Adaptation (DRDA) which bridges source and target domains via a shared radial structure. It’s motivated by the observation that as the model is trained to be progressively discriminative, features of different categories expand outwards in different directions, forming a radial structure. We show that transferring such an inherently discriminative structure would enable to enhance feature transferability and discriminability simultaneously. Speciﬁcally, we represent each domain with a global anchor and each category a local anchor to form a radial structure and reduce domain shift via structure matching. It consists of two parts, namely isometric transformation to align the structure globally and local reﬁnement to match each category. To enhance the discriminability of the structure, we further encourage samples to cluster close to the corresponding local anchors based on optimal-transport assignment. Extensively experimenting on multiple benchmarks, our method is shown to consistently outperforms state-of-the-art approaches on varied tasks, including the typical unsupervised domain adaptation, multi-source domain adaptation, domainagnostic learning, and domain generalization.},
  selected={true}
}

@inproceedings{huangIDAGInvariantDAG2023,
  bibtex_show={true},
  abbr={ICCV},
  title = {{{iDAG}}: {{Invariant DAG Searching}} for {{Domain Generalization}}},
  shorttitle = {{{iDAG}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Huang, Zenan and Wang, Haobo and Zhao, Junbo and Zheng, Nenggan},
  year = {2023},
  pages = {19169--19179},
  urldate = {2023-11-28},
  copyright = {All rights reserved},
  html = {/projects/003_idag},
  langid = {english},
  abstract = {Existing machine learning (ML) models are often fragile in open environments because the data distribution frequently shifts. To address this problem, domain generalization (DG) aims to explore underlying invariant patterns for stable prediction across domains. In this work, we first characterize that this failure of conventional ML models in DG attributes to an inadequate identification of causal structures. We further propose a novel invariant Directed Acyclic Graph (dubbed iDAG) searching framework that attains an invariant graphical relation as the proxy to the causality structure from the intrinsic data-generating process. To enable tractable computation, iDAG solves a constrained optimization objective built on a set of representative class-conditional prototypes. Additionally, we integrate a hierarchical contrastive learning module, which poses a strong effect of clustering, for enhanced prototypes as well as stabler prediction. Extensive experiments on the synthetic and real-world benchmarks demonstrate that iDAG outperforms the state-of-the-art approaches, verifying the superiority of causal structure identification for DG. The code of iDAG is available at https://github.com/lccurious/iDAG.},
  selected={true}
}

@inproceedings{huangLatentProcessesIdentification2023,
  bibtex_show={true},
  abbr={IJCAI},
  title = {Latent {{Processes Identification From Multi-View Time Series}}},
  booktitle = {Thirty-{{Second International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Huang, Zenan and Wang, Haobo and Zhao, Junbo and Zheng, Nenggan},
  year = {2023},
  month = aug,
  volume = {4},
  pages = {3848--3856},
  issn = {1045-0823},
  doi = {10.24963/ijcai.2023/428},
  urldate = {2023-08-27},
  copyright = {All rights reserved},
  html = {/projects/002_multi},
  langid = {english},
  abstract = {Understanding the dynamics of time series data typically requires identifying the unique latent factors for data generation, a.k.a., latent processes identification. Driven by the independent assumption, existing works have made great progress in handling single-view data. However, it is a nontrivial problem that extends them to multi-view time series data because of two main challenges: (i) the complex data structure, such as temporal dependency, can result in violation of the independent assumption; (ii) the factors from different views are generally overlapped and are hard to be aggregated to a complete set. In this work, we propose a novel framework MuLTI that employs the contrastive learning technique to invert the data generative process for enhanced identifiability. Additionally, MuLTI integrates a permutation mechanism that merges corresponding overlapped variables by the establishment of an optimal transport formula. Extensive experimental results on synthetic and real-world datasets demonstrate the superiority of our method in recovering identifiable latent variables on multi-view time series. The code is available on https://github.com/lccurious/MuLTI.},
  selected={true}
}

@inproceedings{liImprovingMovementRelatedCortical2021,
  bibtex_show={true},
  abbr={IEEE NER},
  title = {Improving {{Movement-Related Cortical Potential Detection}} at the {{EEG Source Domain}}},
  booktitle = {2021 10th {{International IEEE}}/{{EMBS Conference}} on {{Neural Engineering}} ({{NER}})},
  author = {Li, Chenyang and Guan, Haonan and Huang, Zenan and Chen, Weidong and Li, Jianhua and Zhang, Shaomin},
  year = {2021},
  month = may,
  pages = {214--217},
  issn = {1948-3554},
  doi = {10.1109/NER49283.2021.9441169},
  urldate = {2023-12-21},
  copyright = {All rights reserved}
}

@inproceedings{pengEnergybasedAutomatedModel2024,
  bibtex_show={true},
  abbr={ICLR},
  title = {Energy-Based {{Automated Model Evaluation}}},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  author = {Peng, Ru and Zou, Heming and Wang, Haobo and Zeng, Yawen and Huang, Zenan and Zhao, Junbo},
  year = {2024},
  urldate = {2024-01-19},
  copyright = {All rights reserved},
  langid = {english},
  abstract = {The conventional evaluation protocols on machine learning models rely heavily on a labeled, i.i.d-assumed testing dataset, which is not often present in real-world applications. The Automated Model Evaluation (AutoEval) shows an alternative to this traditional workflow, by forming a proximal prediction pipeline of the testing performance without the presence of ground-truth labels. Despite its recent successes, the AutoEval frameworks still suffer from an overconfidence issue, substantial storage and computational cost. In that regard, we propose a novel measure --- Meta-Distribution Energy (MDE) that allows the AutoEval framework to be both more efficient and effective. The core of the MDE is to establish a meta-distribution statistic, on the information (energy) associated with individual samples, then offer a smoother representation enabled by energy-based learning. We further provide our theoretical insights by connecting the MDE with the classification loss. We provide extensive experiments across modalities, datasets and different architectural backbones to validate MDE's validity, together with its superiority compared with prior approaches. We also prove MDE's versatility by showing its seamless integration with large-scale models, and easy adaption to learning scenarios with noisy- or imbalanced- labels.}
}

@article{jiangMCAMomentChannel2024,
  bibtex_show={true},
  abbr={AAAI},
  title = {{{MCA}}: {{Moment Channel Attention Networks}}},
  shorttitle = {{{MCA}}},
  author = {Jiang, Yangbo and Jiang, Zhiwei and Han, Le and Huang, Zenan and Zheng, Nenggan},
  year = {2024},
  month = mar,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {38},
  number = {3},
  pages = {2579--2588},
  issn = {2374-3468},
  doi = {10.1609/aaai.v38i3.28035},
  urldate = {2024-05-07},
  copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  abstract = {Channel attention mechanisms endeavor to recalibrate channel weights to enhance representation abilities of networks. However, mainstream methods often rely solely on global average pooling as the feature squeezer, which significantly limits the overall potential of models. In this paper, we investigate the statistical moments of feature maps within a neural network. Our findings highlight the critical role of high-order moments in enhancing model capacity. Consequently, we introduce a flexible and comprehensive mechanism termed Extensive Moment Aggregation (EMA) to capture the global spatial context. Building upon this mechanism, we propose the Moment Channel Attention (MCA) framework, which efficiently incorporates multiple levels of moment-based information while minimizing additional computation costs through our Cross Moment Convolution (CMC) module. The CMC module via channel-wise convolution layer to capture multiple order moment information as well as cross channel features. The MCA block is designed to be lightweight and easily integrated into a variety of neural network architectures. Experimental results on classical image classification, object detection, and instance segmentation tasks demonstrate that our proposed method achieves state-of-the-art results, outperforming existing channel attention methods.}
}

@inproceedings{xiaUnbiasedMultiLabelLearning2024,
  bibtex_show={true},
  abbr={ICML},
  author={Xia, Mingxuan and Huang, Zenan and Wu, Runze and Lyu, Gengyu and Zhao, Junbo and Chen, Gang and Wang, Haobo},
  booktitle={The {{Forty-first International Conference}} on {{Machine Learning}}},
  title={Unbiased Multi-Label Learning from Crowdsourced Annotations},
  year={2024},
  abstract={This work studies the novel Crowdsourced Multi-Label Learning (CMLL) problem, where each instance is related to multiple true labels but the model only receives unreliable labels from different annotators. Although a few Crowdsourced Multi-Label Inference (CMLI) methods have addressed learning with multiple labels under crowdsourcing, they pay more attention to directly identifying true labels given crowdsourced ones and lack of theoretical guarantees of the learned multi-label predictor. In this paper, by excavating the generation process of crowdsourced labels, we establish the first \textbf{unbiased risk estimator} for CMLL based on the crowdsourced transition matrices. To facilitate transition matrix estimation, we upgrade our unbiased risk estimator by aggregating crowdsourced labels and transition matrices from all annotators while guaranteeing its theoretical characteristics. Integrating with the unbiased risk estimator, we further propose a decoupled autoencoder framework to exploit label correlations and boost performance. We also provide a generalization error bound to ensure the convergence of the empirical risk estimator. Experiments on various CMLL scenarios demonstrate the effectiveness of our proposed method.}
}

@misc{wenInterventionalDomainAdaptation2020,
  bibtex_show={true},
  title = {Interventional {{Domain Adaptation}}},
  author = {Wen, Jun and Shui, Changjian and Kuang, Kun and Yuan, Junsong and Huang, Zenan and Gong, Zhefeng and Zheng, Nenggan},
  year = {2020},
  month = nov,
  number = {arXiv:2011.03737},
  eprint = {2011.03737},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2011.03737},
  urldate = {2022-10-27},
  archiveprefix = {arxiv},
  copyright = {All rights reserved}
}

@inproceedings{xiaSeparationAlignmentFramework2024,
  bibtex_show={true},
  abbr={AAAI},
  title = {A {{Separation}} and {{Alignment Framework}} for {{Black-box Domain Adaptation}}},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Xia, Mingxuan and Zhao, Junbo and Gengyu, Lyu and Huang, Zenan and Hu, Tianlei and Chen, Gang and Wang, Haobo},
  year = {2024},
  copyright = {All rights reserved},
  abstract = {Black-box domain adaptation (BDA) targets to learn a classifier on an unsupervised target domain while assuming only access to black-box predictors trained from unseen source data. Although a few BDA approaches have demonstrated promise by manipulating the transferred labels, they largely overlook the rich underlying structure in the target domain. To address this problem, we introduce a novel separation and alignment framework for BDA. Firstly, we locate those well-adapted samples via loss ranking and a flexible confidence-thresholding procedure. Then, we introduce a novel graph contrastive learning objective that aligns under-adapted samples to their local neighbors and well-adapted samples. Lastly, the adaptation is finally achieved by a nearest-centroid-augmented objective that exploits the clustering effect in the feature space. Extensive experiments demonstrate that our proposed method outperforms best baselines on benchmark datasets, e.g. improving the averaged per-class accuracy by 4.1% on the VisDA dataset. The source code is available at: https://github.com/MingxuanXia/SEAL.}
}

@misc{zhouRepresentationDrosophilaLarval2021,
  bibtex_show={true},
  title = {Representation of {{Drosophila}} Larval Behaviors by Muscle Activity Patterns},
  author = {Zhou, Jinrun and Huang, Zenan and Li, Xinhang and Song, Zhiying and Sun, Yixuan and Ping, Junyu and Chen, Xiaopeng and Fei, Peng and Zheng, Nenggan and Gong, Zhefeng},
  year = {2021},
  month = nov,
  primaryclass = {New Results},
  pages = {2021.11.26.470133},
  publisher = {{bioRxiv}},
  doi = {10.1101/2021.11.26.470133},
  urldate = {2022-09-27},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english}
}
