---
---

@string{aps = {American Physical Society,}}

@article{huangDiscriminativeRadialDomain2023,
  bibtex_show={true},
  abbr={IEEE TIP},
  ids = {huangDiscriminativeRadialDomain2023a},
  title = {Discriminative {{Radial Domain Adaptation}}},
  author = {Huang, Zenan and Wen, Jun and Chen, Siheng and Zhu, Linchao and Zheng, Nenggan},
  year = {2023},
  journal = {IEEE Transactions on Image Processing},
  pages = {1--1},
  issn = {1941-0042},
  doi = {10.1109/TIP.2023.3235583},
  copyright = {All rights reserved},
  html={/projects/001_drda},
  abstract={Domain adaptation methods reduce domain shift typically by learning domain-invariant features. Most existing methods are built on distribution matching, e.g., adversarial domain adaptation, which tends to corrupt feature discriminability. In this paper, we propose Discriminative Radial Domain Adaptation (DRDA) which bridges source and target domains via a shared radial structure. It’s motivated by the observation that as the model is trained to be progressively discriminative, features of different categories expand outwards in different directions, forming a radial structure. We show that transferring such an inherently discriminative structure would enable to enhance feature transferability and discriminability simultaneously. Speciﬁcally, we represent each domain with a global anchor and each category a local anchor to form a radial structure and reduce domain shift via structure matching. It consists of two parts, namely isometric transformation to align the structure globally and local reﬁnement to match each category. To enhance the discriminability of the structure, we further encourage samples to cluster close to the corresponding local anchors based on optimal-transport assignment. Extensively experimenting on multiple benchmarks, our method is shown to consistently outperforms state-of-the-art approaches on varied tasks, including the typical unsupervised domain adaptation, multi-source domain adaptation, domainagnostic learning, and domain generalization.},
  selected={true}
}

@inproceedings{huangIDAGInvariantDAG2023,
  bibtex_show={true},
  abbr={ICCV},
  title = {{{iDAG}}: {{Invariant DAG Searching}} for {{Domain Generalization}}},
  shorttitle = {{{iDAG}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Huang, Zenan and Wang, Haobo and Zhao, Junbo and Zheng, Nenggan},
  year = {2023},
  pages = {19169--19179},
  urldate = {2023-11-28},
  copyright = {All rights reserved},
  html = {/projects/003_idag},
  langid = {english},
  abstract = {Existing machine learning (ML) models are often fragile in open environments because the data distribution frequently shifts. To address this problem, domain generalization (DG) aims to explore underlying invariant patterns for stable prediction across domains. In this work, we first characterize that this failure of conventional ML models in DG attributes to an inadequate identification of causal structures. We further propose a novel invariant Directed Acyclic Graph (dubbed iDAG) searching framework that attains an invariant graphical relation as the proxy to the causality structure from the intrinsic data-generating process. To enable tractable computation, iDAG solves a constrained optimization objective built on a set of representative class-conditional prototypes. Additionally, we integrate a hierarchical contrastive learning module, which poses a strong effect of clustering, for enhanced prototypes as well as stabler prediction. Extensive experiments on the synthetic and real-world benchmarks demonstrate that iDAG outperforms the state-of-the-art approaches, verifying the superiority of causal structure identification for DG. The code of iDAG is available at https://github.com/lccurious/iDAG.},
  selected={true}
}

@inproceedings{huangLatentProcessesIdentification2023,
  bibtex_show={true},
  abbr={IJCAI},
  title = {Latent {{Processes Identification From Multi-View Time Series}}},
  booktitle = {Thirty-{{Second International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Huang, Zenan and Wang, Haobo and Zhao, Junbo and Zheng, Nenggan},
  year = {2023},
  month = aug,
  volume = {4},
  pages = {3848--3856},
  issn = {1045-0823},
  doi = {10.24963/ijcai.2023/428},
  urldate = {2023-08-27},
  copyright = {All rights reserved},
  html = {/projects/002_multi},
  langid = {english},
  abstract = {Understanding the dynamics of time series data typically requires identifying the unique latent factors for data generation, a.k.a., latent processes identification. Driven by the independent assumption, existing works have made great progress in handling single-view data. However, it is a nontrivial problem that extends them to multi-view time series data because of two main challenges: (i) the complex data structure, such as temporal dependency, can result in violation of the independent assumption; (ii) the factors from different views are generally overlapped and are hard to be aggregated to a complete set. In this work, we propose a novel framework MuLTI that employs the contrastive learning technique to invert the data generative process for enhanced identifiability. Additionally, MuLTI integrates a permutation mechanism that merges corresponding overlapped variables by the establishment of an optimal transport formula. Extensive experimental results on synthetic and real-world datasets demonstrate the superiority of our method in recovering identifiable latent variables on multi-view time series. The code is available on https://github.com/lccurious/MuLTI.},
  selected={true}
}

@inproceedings{liImprovingMovementRelatedCortical2021,
  bibtex_show={true},
  abbr={IEEE NER},
  title = {Improving {{Movement-Related Cortical Potential Detection}} at the {{EEG Source Domain}}},
  booktitle = {2021 10th {{International IEEE}}/{{EMBS Conference}} on {{Neural Engineering}} ({{NER}})},
  author = {Li, Chenyang and Guan, Haonan and Huang, Zenan and Chen, Weidong and Li, Jianhua and Zhang, Shaomin},
  year = {2021},
  month = may,
  pages = {214--217},
  issn = {1948-3554},
  doi = {10.1109/NER49283.2021.9441169},
  urldate = {2023-12-21},
  copyright = {All rights reserved}
}

@inproceedings{pengEnergybasedAutomatedModel2023,
  bibtex_show={true},
  abbr={ICLR},
  title = {Energy-Based {{Automated Model Evaluation}}},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  author = {Peng, Ru and Zou, Heming and Wang, Haobo and Zeng, Yawen and Huang, Zenan and Zhao, Junbo},
  year = {2024},
  month = oct,
  urldate = {2024-01-19},
  copyright = {All rights reserved},
  langid = {english}
}

@misc{wenInterventionalDomainAdaptation2020,
  bibtex_show={true},
  title = {Interventional {{Domain Adaptation}}},
  author = {Wen, Jun and Shui, Changjian and Kuang, Kun and Yuan, Junsong and Huang, Zenan and Gong, Zhefeng and Zheng, Nenggan},
  year = {2020},
  month = nov,
  number = {arXiv:2011.03737},
  eprint = {2011.03737},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2011.03737},
  urldate = {2022-10-27},
  archiveprefix = {arxiv},
  copyright = {All rights reserved}
}

@inproceedings{xiaSeparationAlignmentFramework2024,
  bibtex_show={true},
  abbr={AAAI},
  title = {A {{Separation}} and {{Alignment Framework}} for {{Black-box Domain Adaptation}}},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}} ({{AAAI}})},
  author = {Xia, Mingxuan and Zhao, Junbo and Gengyu, Lyu and Huang, Zenan and Hu, Tianlei and Chen, Gang and Wang, Haobo},
  year = {2024},
  copyright = {All rights reserved}
}

@misc{zhouRepresentationDrosophilaLarval2021,
  bibtex_show={true},
  title = {Representation of {{Drosophila}} Larval Behaviors by Muscle Activity Patterns},
  author = {Zhou, Jinrun and Huang, Zenan and Li, Xinhang and Song, Zhiying and Sun, Yixuan and Ping, Junyu and Chen, Xiaopeng and Fei, Peng and Zheng, Nenggan and Gong, Zhefeng},
  year = {2021},
  month = nov,
  primaryclass = {New Results},
  pages = {2021.11.26.470133},
  publisher = {{bioRxiv}},
  doi = {10.1101/2021.11.26.470133},
  urldate = {2022-09-27},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english}
}
