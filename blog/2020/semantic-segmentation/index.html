<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> 图像语义分割 | Curiousity Hub </title> <meta name="author" content="Zenan Huang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="causal-learning, causal-discovery, transfer-learning, LLMs, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon-32x32-next-my.png?b29ea1d332f6bc3c021e15d78d266303"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://lccurious.github.io/blog/2020/semantic-segmentation/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Curiousity Hub </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/cv/">cv</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">图像语义分割</h1> <p class="post-meta"> Created in February 09, 2020 </p> <p class="post-tags"> <a href="/blog/2020"> <i class="fa-solid fa-calendar fa-sm"></i> 2020 </a>   ·   <a href="/blog/tag/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2"> <i class="fa-solid fa-hashtag fa-sm"></i> 图像分割</a>   <a href="/blog/tag/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2"> <i class="fa-solid fa-hashtag fa-sm"></i> 语义分割</a>   ·   <a href="/blog/category/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"> <i class="fa-solid fa-tag fa-sm"></i> 机器学习</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>图像分割，或者叫语义图像分割，通过给图像的每一个像素分配一个它所属的类别标签。通过这种分割可以把图像中语义抽象表示出来，也是计算机视觉中一个非常重要的方向，有着非常众多重要应用场景，包括自动驾驶中的场景分割，医学影像中的病灶区域分割等。</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2020-02-09-semantic-segmentation/abs-480.webp 480w,/assets/img/2020-02-09-semantic-segmentation/abs-800.webp 800w,/assets/img/2020-02-09-semantic-segmentation/abs-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/2020-02-09-semantic-segmentation/abs.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="abs" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h1 id="传统方法">传统方法</h1> <p>称传统方法是为了区别于基于深度学习的方法，主要由很多手工设计的显式特征提取算法构成算法的主要部分。</p> <h2 id="阈值方法">阈值方法</h2> <p>在深度学习广泛占据视角之前，经常通过手工设计的特征检测方法对图像进行处理，包括通过阈值分割，把颜色深的文字和颜色浅的背景分割开来，基于这种思想，又发展出多阈值分割，动态阈值分割，将智能遗传算法应用在局部阈值的选取上等方法。但是总体而言这种基于阈值分割的方法计算量低，对噪声敏感。</p> <h2 id="局部生长">局部生长</h2> <p>基于同一个目标物有相似的图像特征，基于选定种子像素点开始向周围生长的方法，不断将和像素相邻的相似区域并入生长区域，并将新添加的像素点作为新的种子点继续向外合并的方法称为区域生长算法。这种方法的关键为：</p> <ol> <li>选择一组有代表性的种子像素</li> <li>用于判断相邻像素是否可以合并的准则</li> <li>算法停止生长的边界条件</li> </ol> <h2 id="区域分裂合并">区域分裂合并</h2> <p>和局部生长算法相对应，是从整幅图像出发，通过不断地分裂得到各个子区域，然后再从子区域集合中选出子集得到需要分割的前景目标。局部生长判断是否可以合并，区域分裂判断是否可以分裂，在实际应用中也常常和局部生长结合使用，但是通常需要考虑目标场景的先验知识，例如光照条件、目标大小等。</p> <h2 id="分水岭算法">分水岭算法</h2> <p>分水岭算法 (watersheld) 和名字相似，可以想象成往沟壑纵横的山岭地形中倒水，低洼地带会积聚水，高耸地带不会被水淹没。针对图像场景，通常用图像中像素值大小来代表地形高低，水从每个局部最低点涌出，都一直涨到刚好快要溢出大坝的高度，这样就可以将图像分割成很多的区域。比如针对细胞照片数据就可以用过这种算法把细胞从背景中分离出来，方便后续对于细胞的统计。</p> <h2 id="基于边缘的检测">基于边缘的检测</h2> <p>通常在图像的目标物之间都会有明显的边缘差异，人们通过这种差异来区分一个物体和另外一个物体，例如区分图像中的黑色的马路和绿色的草地有明显的边缘。人们设计出多种边缘检测算子包括一阶梯度，Roberts算子，Sobel算子，Prewit算子，Kirsh算子，Lapalacian算子等，用于不同场景下的边缘检测。但是边缘检测常常不能产生连续的边缘，在此基础上又有了各种动态边缘检测的方法保证获取更加光滑准确的边缘。</p> <h2 id="主动轮廓模型">主动轮廓模型</h2> <p>主动轮廓模型有统一开放的描述方式，可以想象给定一个初始的闭合曲线，通过设计的内部能量和外部能量计算函数，想象该曲线会同时受到向内和向外的力，力的大小和方向取决于图像数据的特征，曲线在这两种力的作用下最终会收敛到一个合适的轮廓。这种动态逼近的方法可以保证曲线是闭合光滑的。</p> <p>Snake模型是一种非常经典的活动模型，曲线受到的内力用于约束曲线形状，外力则用于牵拉模型到特征轮廓位置，这两种力最终引导曲线能很好地贴合图像的视觉分割边界。</p> <h1 id="深度方法">深度方法</h1> <p>深度卷积神经网络有着非常高的特征提取能力，因此子能够为图像分割提供极为丰富的特征，有时候也和传统方法结合以获得更好的目标特征建模能力。例如 VGGNet 和 ResNet 都是性能非常卓越的特征提取网络，也广泛用于图像分类，目标跟踪，图像语义分割等应用实例的基础特征提取网络中。常见的做法为，保持特征提取基础网络不变，替换模型最后几层的网络结构为特定任务的网络结构，然后通过任务指定的优化目标对整个网络进行重新训练。</p> <p>现在常见的两种语义分割的任务包括：</p> <ul> <li> <strong>类别预测</strong>：估计出每一个像素所属的类别</li> <li> <strong>目标分割</strong>：估计出每个像素所属的目标，用于区分场景中不同的个体</li> </ul> <h2 id="全卷积网络">全卷积网络</h2> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2020-02-09-semantic-segmentation/FCN-480.webp 480w,/assets/img/2020-02-09-semantic-segmentation/FCN-800.webp 800w,/assets/img/2020-02-09-semantic-segmentation/FCN-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/2020-02-09-semantic-segmentation/FCN.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="FCN" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>全卷积的网络，通过不断的卷积采样，获得高维的特征，然后通过上采样（线性插值，反卷积等）将分辨率低的特征图映射成和输入图像大小相同的分割图像，再通过损失函数对网络中的参数进行优化，在同一个输出特征图上同时计算分割和分类的误差。</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2020-02-09-semantic-segmentation/UNet-480.webp 480w,/assets/img/2020-02-09-semantic-segmentation/UNet-800.webp 800w,/assets/img/2020-02-09-semantic-segmentation/UNet-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/2020-02-09-semantic-segmentation/UNet.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="UNet" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>U-Net 通过卷积和下采样获得更高级别的语义信息，然后通过上采样的方式得到和输入图像大小一致的类别分割图。为了保持低级的信息也能继续保持在最终的输出结果中，原图进行下采样时也会将对应的特征图和上采样后得到的对应相同大小的特征图进行拼接，使得最终的输出结果有更加精细的分割效果。</p> <h2 id="基于区域选择">基于区域选择</h2> <p>也就是基于 RPN（Region Proposal Network）的思想，来自于目标检测任务的解决思路，通过低级的特征，例如颜色空间的相似矩阵等，算法根据这些结果预先提出 2000 个候选框，然后把每个框内的内容输入分类网络，最终只留下几个分类模型最确定的框来表示目标的位置和类别，但是这种做法有非常明显的性能劣势。Fast R-CNN 的提出就是为了改进这其中非常耗时的超多候选框判断问题，直接通过特征图生成图像中各个网格区域的特征，然后直接得到对应的分类置信度对这些网格进行重新组合，得到一个修正的包围框表示这个目标的位置。在此基础上又有了改进方案 Faster R-CNN，用 RPN 网络进行候选区域的提取。</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2020-02-09-semantic-segmentation/RPN-480.webp 480w,/assets/img/2020-02-09-semantic-segmentation/RPN-800.webp 800w,/assets/img/2020-02-09-semantic-segmentation/RPN-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/2020-02-09-semantic-segmentation/RPN.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="RPN" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>通过以每个特征图的像素点为中心生成这些密密麻麻的候选网格，用一个二分类器判断每个网格是否有用，从而能够降低整体的计算开销，快速得到各个目标的位置和类别。</p> <p>Mask R-CNN 分割方法也就是建立在这些候选框提出到分类的工作基础上，用的是多分枝的结构，同时输出物体的类别、包围框和蒙版，用的是二值判断确定各个像素是否属于某个物体类别。主要的贡献有：</p> <ul> <li>在 Faster R-CNN 的基础上，对于 Faster-RCNN 提出的每个候选包围框都用 FCN 进行语义分割，同时实现分割，定位，分类三个任务</li> <li>引入 ROI Align 代替基于池化的粗超网格划分方法，提高了分类细粒度，可以极大提高目标蒙版的精度</li> <li>用多个分支，Mask 分支只做语义分割，类别分支制作类别预测，和原始的 FCN 不同</li> </ul> <p>针对 ROI Align，因为之前的 Faster R-CNN 方法是给特征上每一个点都提出 9 个大小形状不同的 ROI 这些不同的 ROI 需要经过 ROI Pooling 才能转化成大小维度一致的输入到下一层网络，而在分割任务中这么做就会损失图像原有的细节特征（因为被形变，长宽比都不对了），所以 ROI Align 通过双线性插值找到每个块对应的特征。</p> <h2 id="特征金字塔">特征金字塔</h2> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2020-02-09-semantic-segmentation/PSN-480.webp 480w,/assets/img/2020-02-09-semantic-segmentation/PSN-800.webp 800w,/assets/img/2020-02-09-semantic-segmentation/PSN-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/2020-02-09-semantic-segmentation/PSN.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="PSN" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>通过卷积神经网络获得输入图像（a）的特征图（b），然后通过不同大小的池化操作获得多个分辨率的特征图（c）然后再通过上采样（放大）的操作将这些特征图组合在一起经过卷积神经网络之后获得最终的输出（d）。</p> <h2 id="条件随机场">条件随机场</h2> <p>条件随机场（CRF，Conditional Random Field）是一种特殊的马尔可夫随机场，马尔可夫随机场（MRF，Marcov Random Field）是一种基于统计的图像分割算法，马尔可夫模型是指一组事件的集合，集合中每个事件的发生仅仅由前一个事件的发生决定，和更早之前发生的事件无关。马尔可夫随机场就代表，任何区域都只和它邻近的区域有关，图像中每个像素（或者更大块的特征）的集合就组成一个马尔可夫随机场。CRF 则是一种在给定一组输入随机变量 \(X\) 有另一组随机变量 \(Y\) 的输出，现实中我们常常假设 \(X\) 和 \(Y\) 有相同的结构 \(X=(x_{1}, x_{2}, \dots, x_{n}),\quad Y=(y_{1}, y_{2}, \dots, y_{n})\) 图像分割就是要找出每个像素点对应的类别，即给定 \(x\) 时求概率最大的 \(y\)： \begin{equation} \hat{y}^{\text{MAP}} = \text{argmax}_{y\in Y}P(y|x) \end{equation}</p> <p>条件随机场符合吉布斯分布<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>: \begin{equation} P(\mathbf{X}=x|\mathbf{I})=\frac{1}{Z(\mathbf{I})}\exp(-E(x|\mathbf{I})) \end{equation} 其中 \(E(x\vert \mathbf{I})\) 表示能量函数 \begin{equation} E(x)=\sum_{i}\theta_{u}(x_{i}) + \sum_{i\le j}\theta_{p}(x_{i},x_{j}) \end{equation} 其中一元势函数 \(\sum_{i}\theta(x_{i})\) 可以是 FCN 的输出，而二元势函数为： \begin{equation} \theta_{ij}(x_{i}, y_{j}) = u(x_{i}, x_{j})\sum^{m}_{m=1}\omega^{(m)}k_{G}^{(m)}(\mathbf{f}_{i}, \mathbf{f}_{j}) \end{equation} 二元势函数描述像素之间的关系，鼓励相似像素分配相同的标签，相差大的像素分配不同的标签，距离的定义由实际相对距离和颜色等差别决定，保证图片尽量在边界处分割，全连接条件随机场值是指每个像素和所有其他像素的区别。</p> <h2 id="评价指标">评价指标</h2> <h3 id="pixel-accuracy">Pixel Accuracy</h3> <p>分类正确的像素点数和所有的像素点数的比例 \begin{equation} PA = \frac{\sum^{k}_{i=0}p_{ii}}{\sum^{k}_{i=0}\sum^{k}_{j=0}p_{ij}} \end{equation}</p> <h3 id="mean-pixel-accuracy-mpa">Mean Pixel Accuracy (MPA)</h3> <p>计算每一类分类正确的像素点数和该类的所有像素点数的比例然后求平均 \begin{equation} MPA=\frac{1}{k}\sum^{k}_{i=0}\frac{p_{ii}}{\sum^{k}_{j=0}p_{ij}} \end{equation}</p> <h3 id="mean-intersection-over-union-miou">Mean Intersection over Union (MIoU)</h3> <p>计算每一类的IoU然后求平均。一类的IoU计算方式如下，例如i=1，\(p_{11}\) 表示true positives，即本属于1类且预测也为1类，\(\sum^{k}_{k=0}p_{1j}\) 表示本属于1类却预测为其他类的像素点数（注意，这里包含了 \(p_{11}\)）。\(\sum^{k}_{j=0}p_{j1}\) 表示本属于其他类却预测为1类的像素点数（注意，这里也包含了 \(p_{11}\) ），在分母处 \(p_{11}\) 计算了两次所以要减去一个 \(p_{11}\) \begin{equation} MIoU = \frac{1}{k+1}\sum^{k}_{i=0}\frac{p_{ii}}{\sum^{k}_{j=0}p_{ij}+\sum^{k}_{i=0}p_{ji}-p_{ii}} \end{equation}</p> <h3 id="frequency-weighted-intersection-over-union-fwiou">Frequency Weighted Intersection over Union (FWIoU)</h3> <p>可以理解为根据每一类出现的频率对各个类的IoU进行加权求和 \begin{equation} FWIoU=\frac{1}{\sum^{k}_{i=0}\sum^{k}_{j=0}p_{ij}}\sum^{k}_{i=0}\frac{p_{ii}\sum^{k}_{j=0}p_{ij}}{\sum^{k}_{j=0}p_{ij}+\sum^{k}_{j=0}p_{ij}-p_{ii}} \end{equation}</p> <h2 id="数据集">数据集</h2> <h3 id="ms-coco">MS COCO</h3> <p>MS于2014年发布的Microsoft COCO数据集，已成为图像字幕的标准测试平台。</p> <p>原来的数据集有20G左右的图片和500M左右的标签文件。标签文件标记了每个segmentation+bounding box（即分割物+分割物的边界）的精确坐标，其精度均为小数点后两位。例如，一个目标分割物的标签示意如下：</p> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"segmentation"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
    </span><span class="p">[</span><span class="w">
      </span><span class="mf">392.87</span><span class="p">,</span><span class="w"> </span><span class="mf">275.77</span><span class="p">,</span><span class="w"> </span><span class="mf">402.24</span><span class="p">,</span><span class="w"> </span><span class="mf">284.2</span><span class="p">,</span><span class="w"> </span><span class="mf">382.54</span><span class="p">,</span><span class="w"> </span><span class="mf">342.36</span><span class="p">,</span><span class="w"> </span><span class="mf">375.99</span><span class="p">,</span><span class="w"> </span><span class="mf">356.43</span><span class="p">,</span><span class="w"> </span><span class="mf">372.23</span><span class="p">,</span><span class="w"> </span><span class="mf">357.37</span><span class="p">,</span><span class="w"> </span><span class="mf">372.23</span><span class="p">,</span><span class="w"> </span><span class="mf">397.7</span><span class="p">,</span><span class="w"> </span><span class="mf">383.48</span><span class="p">,</span><span class="w"> </span><span class="mf">419.27</span><span class="p">,</span><span class="w"> </span><span class="mf">407.87</span><span class="p">,</span><span class="w"> </span><span class="mf">439.91</span><span class="p">,</span><span class="w"> </span><span class="mf">427.57</span><span class="p">,</span><span class="w"> </span><span class="mf">389.25</span><span class="p">,</span><span class="w">
      </span><span class="mf">447.26</span><span class="p">,</span><span class="w"> </span><span class="mf">346.11</span><span class="p">,</span><span class="w"> </span><span class="mf">447.26</span><span class="p">,</span><span class="w"> </span><span class="mf">328.29</span><span class="p">,</span><span class="w"> </span><span class="mf">468.84</span><span class="p">,</span><span class="w"> </span><span class="mf">290.77</span><span class="p">,</span><span class="w"> </span><span class="mf">472.59</span><span class="p">,</span><span class="w"> </span><span class="mf">266.38</span><span class="w">
    </span><span class="p">],</span><span class="w">
    </span><span class="p">[</span><span class="mf">429.44</span><span class="p">,</span><span class="w"> </span><span class="mf">465.23</span><span class="p">,</span><span class="w"> </span><span class="mf">453.83</span><span class="p">,</span><span class="w"> </span><span class="mf">473.67</span><span class="p">,</span><span class="w"> </span><span class="mf">636.73</span><span class="p">,</span><span class="w"> </span><span class="mf">474.61</span><span class="p">,</span><span class="w"> </span><span class="mf">636.73</span><span class="p">,</span><span class="w"> </span><span class="mf">392.07</span><span class="p">,</span><span class="w"> </span><span class="mf">571.07</span><span class="p">,</span><span class="w"> </span><span class="mf">364.88</span><span class="p">,</span><span class="w"> </span><span class="mf">546.69</span><span class="p">,</span><span class="w"> </span><span class="mf">363.0</span><span class="p">]</span><span class="w">
  </span><span class="p">],</span><span class="w">
  </span><span class="nl">"area"</span><span class="p">:</span><span class="w"> </span><span class="mf">28458.996150000003</span><span class="p">,</span><span class="w">
  </span><span class="nl">"iscrowd"</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w">
  </span><span class="nl">"image_id"</span><span class="p">:</span><span class="w"> </span><span class="mi">503837</span><span class="p">,</span><span class="w">
  </span><span class="nl">"bbox"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">372.23</span><span class="p">,</span><span class="w"> </span><span class="mf">266.38</span><span class="p">,</span><span class="w"> </span><span class="mf">264.5</span><span class="p">,</span><span class="w"> </span><span class="mf">208.23</span><span class="p">],</span><span class="w">
  </span><span class="nl">"category_id"</span><span class="p">:</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w">
  </span><span class="nl">"id"</span><span class="p">:</span><span class="w"> </span><span class="mi">151109</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> <p>数据集以场景理解为目标，主要从复杂的日常场景中截取，图像中的目标通过精确的segmentation（分割）进行位置的标定。</p> <h3 id="ade20k-数据集">ADE20K 数据集</h3> <p>ADE20K 是用来做 scene parsing 的一个非常大的数据集合，包含 150 中物体类型，由 MIT CSAIL 研究组维护。数据集主页在 http://groups.csail.mit.edu/vision/datasets/ADE20K/</p> <hr> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p><a href="https://zh.wikipedia.org/zh-hans/%E5%90%89%E5%B8%83%E6%96%AF%E9%87%87%E6%A0%B7" rel="external nofollow noopener" target="_blank">吉布斯采样-维基</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p> </li> </ol> </div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Extend-LLMs-Context-Window/">Extend LLMs Context Window</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/Introduction-to-LLMs/">Introduction to LLMs</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/MST-Analysis/">MST-Analysis</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/Search-Skills/">搜索技巧</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/Word-Tricks/">Word 排版技巧</a> </li> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'lccurious/lccurious.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Zenan Huang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>