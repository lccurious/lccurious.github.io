<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Extend LLMs Context Window | Curiousity Hub </title> <meta name="author" content="Zenan Huang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="causal-learning, causal-discovery, transfer-learning, LLMs, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon-32x32-next-my.png?b29ea1d332f6bc3c021e15d78d266303"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://lccurious.github.io/blog/2024/Extend-LLMs-Context-Window/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Curiousity Hub </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/cv/">cv</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Extend LLMs Context Window</h1> <p class="post-meta"> February 25, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/llms"> <i class="fa-solid fa-hashtag fa-sm"></i> LLMs</a>   <a href="/blog/tag/context-window"> <i class="fa-solid fa-hashtag fa-sm"></i> Context Window</a>     ·   <a href="/blog/category/machine-learning"> <i class="fa-solid fa-tag fa-sm"></i> Machine Learning</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="introduction">Introduction</h2> <p>A large context window is an absolutely desirable feature in Large Language Models (LLMs). However, enhancing these models’ abilities during pretraining period requires costly data collection and significant memory resources, which grow quadratically. Consequently, in the past year, large context window technology has become a highly contested field, drawing significant attention and debate. There are two desired properties: the input context length and the model output length. Generally, more focus is placed on the input context length, as few individuals, apart from those wishing LLMs to compose lengthy narratives, are interested in reading extensive output contexts.</p> <p>Large context conditions are extremely important during the Retrieval-Augmented Generation (RAG) process because we aim for LLMs to read comprehensive materials and either output a perfect summary or utilize the key ideas from retrieved references. Indeed, while some models claim to read entire books, ongoing development has led to the expectation that, on the other hand, models be able to process a list of books, extensive tables, and more. Therefore, LLMs are on track to become an indispensable part of the workflow.</p> <p>However, with limited resources at hand, most users are unable to retrain or fine-tune the models. Therefore, there is an urgent demand for creating resource-efficient adaptation and inference methods. These methods should ideally eliminate the need for retraining or supervised fine-tuning (SFT).</p> <p>There are three sounds similar lines:</p> <ol> <li> <p>Length Extrapolation</p> </li> <li> <p>Context Window Extension</p> </li> <li> <p>Improving LLMs’ Utilization of Long Text</p> </li> </ol> <h2 id="previous-limitation">Previous Limitation</h2> <p>As demonstrated in the <a href="http://arxiv.org/abs/2306.15595" rel="external nofollow noopener" target="_blank">📑Chen et al., 2023</a>, they use <code class="language-plaintext highlighter-rouge">coeffs</code> linear fitting task show the abnormal attention amplitude appeared in the position exceed the pre-training context window size.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-02-25-Extend-LLMs-Context-Window/c5930799_Untitled-480.webp 480w,/assets/img/2024-02-25-Extend-LLMs-Context-Window/c5930799_Untitled-800.webp 800w,/assets/img/2024-02-25-Extend-LLMs-Context-Window/c5930799_Untitled-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-02-25-Extend-LLMs-Context-Window/c5930799_Untitled.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Untitled" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="methodology">Methodology</h2> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-02-25-Extend-LLMs-Context-Window/c24e744a_Untitled-480.webp 480w,/assets/img/2024-02-25-Extend-LLMs-Context-Window/c24e744a_Untitled-800.webp 800w,/assets/img/2024-02-25-Extend-LLMs-Context-Window/c24e744a_Untitled-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-02-25-Extend-LLMs-Context-Window/c24e744a_Untitled.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Untitled" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>As what discussed in the <a href="http://arxiv.org/abs/2306.15595" rel="external nofollow noopener" target="_blank">📑Chen et al., 2023</a>, models often experience a catastrophic drop in performance when the input context window is directly extended. The root cause of this issue lies in the behavior of attention scores, which can become erratic due to encountering unexpected values—typically those exceeding the maximum context length used during the pretraining phase. This erratic behavior can lead to the generation of disproportionately large attention coefficients, thereby degrading the model’s representation capabilities. To mitigate this issue, <a href="http://arxiv.org/abs/2306.15595" rel="external nofollow noopener" target="_blank">📑Chen et al., 2023</a> proposed an on-the-fly context re-distribution interpolation method that effectively prevents these abnormal jumps in attention scores:</p> \[\begin{equation} \mathbf{f}'(\boldsymbol{x},m)=\mathbf{f}\left( \boldsymbol{x},\frac{mL}{L'} \right), \end{equation}\] <p>Where \(L’\) represents the new maximum context window length. Using this scaling method, the position of each new input token can be accurately mapped into the original feasible numerical region, ensuring compatibility with the model’s pretrained structure.</p> <p>Currently, no fine-tuning-free methods are available for this adjustment. However, LongRoPE <a href="http://arxiv.org/abs/2402.13753" rel="external nofollow noopener" target="_blank">📑Ding et al., 2024</a> extends the context length by rescaling the RoPE positional encoding <a href="http://arxiv.org/abs/2104.09864" rel="external nofollow noopener" target="_blank">📑Su et al., 2023</a> frequencies, allowing for a gradual extension of the context window. For a large language model (LLM) targeting a context window size of \(L’\) and processing a lengthy input document \(\boldsymbol{X}\),</p> \[\begin{equation} \mathop{\arg\min}_{\boldsymbol{x}\in \boldsymbol{X};\vert \boldsymbol{x}\vert \geq L'}\mathcal{L}(LLM(RoPE,\boldsymbol{X})) \end{equation}\] <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-02-25-Extend-LLMs-Context-Window/9448554a_Untitled-480.webp 480w,/assets/img/2024-02-25-Extend-LLMs-Context-Window/9448554a_Untitled-800.webp 800w,/assets/img/2024-02-25-Extend-LLMs-Context-Window/9448554a_Untitled-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-02-25-Extend-LLMs-Context-Window/9448554a_Untitled.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Untitled" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Given a position index \(m\in [0,c)\) and an embedding vector \(\boldsymbol{x}:=[x_{0}, x_{1}, \dots, x_{d-1}]^{\top}\), where \(d\) is the dimension of the attention head, RoPE <a href="http://arxiv.org/abs/2104.09864" rel="external nofollow noopener" target="_blank">📑Su et al., 2023</a> defines a vector-valued complex function \(\boldsymbol{f}(\boldsymbol{x},m)\) as follows:</p> \[\begin{equation} \boldsymbol{f}(\boldsymbol{x},m)=[(x_{0}, ix_{1})e^{im\theta_{0}},(x_{2}, ix_{3})e^{im\theta_{1}},\dots,(x_{d-2}, ix_{d-1})e^{im\theta_{d/2-1}}]^{\top} \end{equation}\] <p>The self-attention score \(a(m,n)=\Re \langle \boldsymbol{f}(\boldsymbol{q},m),\boldsymbol{f}(\boldsymbol{k},n)\rangle =\dots =:a(m-n)\), thus enabling the effective use of the relative distance score equivalent in subsequent derivations.</p> <h3 id="uniformly-distribute-the-4096-into-2048-position-grids">Uniformly distribute the 4096 into 2048 position grids</h3> <p>As what they discussed in the <a href="http://arxiv.org/abs/2306.15595" rel="external nofollow noopener" target="_blank">📑Chen et al., 2023</a>, they dive into the detail of the position grids that interpolation between two grid \(s_{1}\) and \(s_{2}\) would not exceed an acceptable bound after fine-tuning on the larger corpus.</p> <blockquote class="block-tip"> <p>💡 How could catastrophic behaviors of ignoring information exceed the content length as attention score \(a_{m-n}\) decays, this distances should not matter that much?</p> </blockquote> <blockquote> <p>It turns out that the upper bound derived in Section 3.4.3 of <a href="http://arxiv.org/abs/2104.09864" rel="external nofollow noopener" target="_blank">📑Su et al., 2023</a> may be too loose: while it indeed decays with respect to \(\vert m-n\vert\), the bound can still be quite large (i.e., the bound can be critically depends on the magnitude of \(v_{j}\)) and thus vacuous.</p> </blockquote> \[\begin{equation} \boldsymbol{f}'(\boldsymbol{x},m)=\boldsymbol{f}\left(\boldsymbol{x},\frac{mL}{L'}\right) \end{equation}\] <h3 id="reason-of-bad-performance-brought-by-the-direct-interpolation">Reason of bad performance brought by the direct interpolation</h3> <p>Let the attention score be denoted as \(a_{m-n}\), which indeed decays with respect to \(\vert m-n\vert\). If we consider all trigonometric functions as basis functions, (i.e., \(\phi_{j}(s):=e^{is\theta_{j}}\)):</p> \[\begin{equation} a(s)=\Re\left[ \sum^{d/2-1}_{j=0}h_{j}e^{is\theta_{j}} \right] \end{equation}\] <p>\(s\) represents the position between a query and key. The complex coefficients \(h_{j}:=(q_{2j}+iq_{2j+1})(k_{2j} - ik_{2j+1})\) depend on \(\boldsymbol{q}\) and \(\boldsymbol{k}\). When \(a(s)\) encounters an unexpected number (usually greater than 2048), it can exhibit unpredictable, abnormal behavior, i.e., output an extremely large coefficient. This negatively impacts the model’s representation capabilities.</p> <blockquote class="block-tip"> <p>💡 Theorem 2.1 (Interpolation bound). For attention score \(a(s) = \Re\left[\sum^{d/2-1}_{j=0}h_{j}e^{is\theta_{j}} \right]\), where \(\theta_{j}=c^{-2j/d}\), its interpolation value \(a(s)\) for \(s\in [s_{1}, s_{2}]\) is bounded as follows \(\begin{equation} \vert a(s)-a_{\rm linear}(s)\vert \leq d\left(\max_{j}\vert h_{j}\vert \right)\frac{(s-s_{1})(s_{2}-s)}{8\ln c} \end{equation}\) where \(a_{\rm linear}(s)\) is the linear interpolation of two grid point \(a(s_{1})\) and \(a(s_{2})\) that are known to behave well, enforced by LLM pre-training: \(\begin{equation} a_{\rm linear}(s):=(1-\lambda(s))a(s_{1})+\lambda(s)a(s_{2}),\quad \lambda(s):=\frac{s-s_{1}}{s_{2}-s_{1}} \end{equation}\)</p> </blockquote> <p>This result ensures that the pre-trained LLMs maintain stable behavior, as their attention does not abruptly shift between grid points \(s_{1}\) and \(s_{2}\). Consequently, this approach achieves a condition where attention scores vary smoothly without causing chaotic amplitude fluctuations.</p> <h3 id="fine-tuning">Fine-tuning</h3> <p>The interpolated model using the the next token prediction task with interpolated position encodings on the extended window size using a pre-training corpus such as the Pile.</p> <p>Training Procedure:</p> <ul> <li> <p>AdamW with \(\beta_{1}=0.9\) and \(\beta_{2}=0.95\).</p> </li> <li> <p>linear learning rate warmup of 20 steps starting from \(10\%\) maximum learning rate.</p> </li> <li> <p>7B and 13B models, LR = \(2\times 10^{-5}\)</p> </li> <li> <p>33B and 65B, LR = \(10^{-5}\)</p> </li> <li> <p>weight decay: 0</p> </li> <li> <p>batch size: 64</p> </li> </ul> <p>More GPUs are required for the large memory needed of large context.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-02-25-Extend-LLMs-Context-Window/8d872c85_Untitled-480.webp 480w,/assets/img/2024-02-25-Extend-LLMs-Context-Window/8d872c85_Untitled-800.webp 800w,/assets/img/2024-02-25-Extend-LLMs-Context-Window/8d872c85_Untitled-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-02-25-Extend-LLMs-Context-Window/8d872c85_Untitled.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Untitled" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="further-discussion">Further discussion</h2> <p><a href="https://www.reddit.com/r/LocalLLaMA/comments/14fgjqj/a_simple_way_to_extending_context_to_8k/" rel="external nofollow noopener" target="_blank">https://www.reddit.com/r/LocalLLaMA/comments/14fgjqj/a_simple_way_to_extending_context_to_8k/</a></p> <p><a href="https://github.com/ggerganov/llama.cpp/discussions/1965" rel="external nofollow noopener" target="_blank">https://github.com/ggerganov/llama.cpp/discussions/1965</a></p> <h2 id="infinite-length-inputs-llms">Infinite-length inputs LLMs</h2> <p><a href="http://arxiv.org/abs/2309.17453" rel="external nofollow noopener" target="_blank">📑Xiao et al., 2024</a> were the first to discover that using an initial token as a placeholder can significantly enhance LLM performance in extremely large contexts, even if the placeholder token does not carry any meaningful information. Although this phenomenon appears counterintuitive to our expectations, it has proven to be effective.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-02-25-Extend-LLMs-Context-Window/d86af3b7_Untitled-480.webp 480w,/assets/img/2024-02-25-Extend-LLMs-Context-Window/d86af3b7_Untitled-800.webp 800w,/assets/img/2024-02-25-Extend-LLMs-Context-Window/d86af3b7_Untitled-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-02-25-Extend-LLMs-Context-Window/d86af3b7_Untitled.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Untitled" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>There are two hypothesis that:</p> <ol> <li> <p>Either initial tokens’ semantics are crucial;</p> </li> <li> <p>the model learns a bias towards their absolute positions;</p> </li> </ol> <p>As attention score is computed by:</p> \[\begin{equation} \mathrm{Attention}(Q,K,V)=\mathrm{Softmax}\left( \frac{QK^{\top}}{\sqrt{d_{k}}} \right)V. \end{equation}\] <p>Consider any location in the sequence corresponding to this attention score matrix, such as the \(i\)-th elements. Let’s examine the attention scores associated with this position:</p> \[\begin{equation} \mathrm{Softmax}(x)_{i}=\frac{e^{x_{i}}}{e^{x_{1}}+\sum^{N}_{j=2}e^{x_{j}}},\quad x_{1}\gg x_{j}, j\in2,\dots, N, \end{equation}\] <p>as you can see, this \(e^{x_{1}}\) would appeared in every later token’s attention scores, it is emmm, almost a constant value. So, why not just make it a true constant value, thereby adapted function looks like:</p> \[\begin{equation} \mathrm{Softmax}_{1}(x)_{i}=\frac{e^{x_{i}}}{1+\sum^{N}_{j=1}e^{x_{j}}},\quad x_{1} \gg x_{j},j\in 2,\dots, N, \end{equation}\] <p>Note that \(j\) starts at 1, with the placeholder token serving solely to absorb unnecessary attention. This formulation does not require the attention scores for contextual tokens to sum to one. By substituting the original softmax function with this modified version, softmax1, the performance degradation remains within acceptable limits.</p> <p>A explanation can be:</p> <blockquote> <p>Consequently, the model tends to dump unnecessary attention values to specific tokens.</p> </blockquote> <blockquote class="block-tip"> <p>📌 Extensive research has been done on applying LLMs to lengthy texts, with three main areas of focus: <strong>Length Extrapolation, Context Window Extension,</strong> and <strong>Improving LLMs’s Utilization of Long Text.</strong> While seemingly related, it’s worth nothing that progress in one direction does’t necessarily lead to progress in the other. This paper does not expand the attention window size of LLMs or enhance the model’s memory and usage on long texts.</p> </blockquote> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-02-25-Extend-LLMs-Context-Window/d6cd64a1_Untitled-480.webp 480w,/assets/img/2024-02-25-Extend-LLMs-Context-Window/d6cd64a1_Untitled-800.webp 800w,/assets/img/2024-02-25-Extend-LLMs-Context-Window/d6cd64a1_Untitled-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-02-25-Extend-LLMs-Context-Window/d6cd64a1_Untitled.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Untitled" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Therefore, we can recall the beginning hypothesis, if the initial token absorb most attention scores? The answer can be:</p> <ul> <li> <p>The initial tokens’ semantics are crucial ❌</p> </li> <li> <p>The model learns a bias towards their absolute position ✅</p> </li> </ul> <h2 id="evaluation">Evaluation</h2> <h3 id="needle-in-a-haystack-test">Needle-in-a-Haystack test</h3> <ol> <li> <p>Place a random fact or statement (the ‘needle’) in the middle of a long context window (the ‘haystack’)</p> </li> <li> <p>Ask the model to retrieve this statement</p> </li> <li> <p>Iterate over various document depths (where the needle is placed) and context lengths to measure performance</p> </li> </ol> <h3 id="passkey-retrieval">Passkey retrieval</h3> <p>In this task, the models are asked to recover a random passkey hidden in a long document. If a model consistently succeeds in retrieving the correct passkey value, means that effective context window size of the model is at least \(k\).</p> <h3 id="long-document-summarization">Long document summarization</h3> <p>There are list of datasets for this evaluation like GovReport, each document comes with a human generated summary. Use ROUGE-1/ROUGE-2/ROUGE-L scores to evaluate the models’ outputs vs the ground-truth summaries.</p> <h3 id="counting-stars">Counting stars</h3> <p>It seems this task lead to a more complex evaluation. Counting-Stars test refers to scattering multiple stars (sentences describing the number of stars) in the sky (a 128K long context), requiring LLMs to collect and summarize them into a specified answer.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-02-25-Extend-LLMs-Context-Window/07cb5d3d_image-480.webp 480w,/assets/img/2024-02-25-Extend-LLMs-Context-Window/07cb5d3d_image-800.webp 800w,/assets/img/2024-02-25-Extend-LLMs-Context-Window/07cb5d3d_image-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-02-25-Extend-LLMs-Context-Window/07cb5d3d_image.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="conclusion">Conclusion</h2> <p>Position Interpolation can effectively extend the context window of LLM models with minimal fine-tuning. The discovery that the “attention sink” plays a critical role in preserving the model’s ability to handle long context windows without performance degradation is particularly noteworthy. This area presents significant opportunities for further exploration.</p> <p><br></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/Word-Tricks/">Word 排版技巧</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/Introduction-to-LLMs/">Introduction to LLMs</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2018/Black-First-Won/">Black First Won</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2018/A-3D-Modeller-zh/">一个3D模型（译）</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/Poincare-Ball-Model/">庞加莱球模型</a> </li> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"lccurious/lccurious.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Zenan Huang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>