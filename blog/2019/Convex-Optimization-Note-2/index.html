<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Convex Optimization Note 2 | Gradient Method | Curiousity Hub </title> <meta name="author" content="Zenan Huang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="causal-learning, causal-discovery, transfer-learning, LLMs, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon-32x32-next-my.png?b29ea1d332f6bc3c021e15d78d266303"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://lccurious.github.io/blog/2019/Convex-Optimization-Note-2/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Curiousity Hub </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/cv/">cv</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Convex Optimization Note 2 | Gradient Method</h1> <p class="post-meta"> April 18, 2019 </p> <p class="post-tags"> <a href="/blog/2019"> <i class="fa-solid fa-calendar fa-sm"></i> 2019 </a>   ·   <a href="/blog/tag/%E5%87%B8%E4%BC%98%E5%8C%96"> <i class="fa-solid fa-hashtag fa-sm"></i> 凸优化</a>   <a href="/blog/tag/%E9%9D%9E%E7%BA%BF%E6%80%A7%E4%BC%98%E5%8C%96"> <i class="fa-solid fa-hashtag fa-sm"></i> 非线性优化</a>     ·   <a href="/blog/category/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"> <i class="fa-solid fa-tag fa-sm"></i> 机器学习</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>在基础背景介绍过之后可以开始非约束的最小化问题的收敛速度（收敛率）（the rate of convergence）学习了，在 post_link “Convex-Optimization-Note-1” 第一章 中也已经证明反梯度方向是最快的下降方向。在这篇文章中，我们将基于基本的直觉感受展开描述。在很多的推导中我们不能以非常直接或者解析的方式去描述某一类问题最为精确的形式，所以我们将尽自己所能找到一些能够有充足证明确定它成立的证明来论证，也就是宁愿给一个不那么精确的描述也不要给出一个错误的估计。下面的笔记依然是基于Lectures on convex optimization<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>。</p> <h2 id="梯度方法">梯度方法</h2> <p>\begin{equation} \begin{array}{l}{\text { Choose } x_{0} \in \mathbb{R}^{n}} \\ {\text { Iterate } x_{k+1}=x_{k}-h_{k} \nabla f\left(x_{k}\right), k=0,1, \ldots}\end{array} \end{equation}</p> <h3 id="步长选择">步长选择</h3> <p>固定步长，也就是每次都按照一个固定的比例把当前时刻的求解得到的梯度作用在搜索的路径上，步长（显然值都为正）的序列 \(\left\{h_{k}\right\}_{k=0}^{\infty}\) 生成规则可以是下面这几种方式：</p> <ul> <li> <strong>固定规则</strong>：这种选择是最简单的，通常在凸优化的情况下选择 \begin{equation} \begin{aligned} h_{k} &amp;=h&gt;0, \text { (constant step } ) \\ h_{k} &amp;=\frac{h}{\sqrt{k+1}} \end{aligned} \end{equation}</li> <li> <strong>完全松弛</strong>：这种看起来就很复杂，每迭代一步就要找这么久，在现实中几乎难以实现这种理想的方式（不过最近几年似乎有了新的进展） \begin{equation} h_{k}=\arg \min _{h \geq 0} f\left(x_{k}-h \nabla f\left(x_{k}\right)\right) \end{equation}</li> <li> <strong>Goldstein-Armijo规则</strong>：找到下一个点 \(x_{k+1}=x_{k}-h \nabla f\left(x_{k}\right)\) 使得： \begin{equation} \begin{array}{l}{\alpha\left\langle\nabla f\left(x_{k}\right), x_{k}-x_{k+1}\right\rangle \leq f\left(x_{k}\right)-f\left(x_{k+1}\right)} \\ {\beta\left\langle\nabla f\left(x_{k}\right), x_{k}-x_{k+1}\right\rangle \geq f\left(x_{k}\right)-f\left(x_{k+1}\right)}\end{array} \end{equation} 其中，\(0&lt;\alpha&lt;\beta&lt;1\) 是事先选择固定的参数。这个规则在大多数实际算法中有所应用而且有几何解释，首先我们写一个以 \(h\) 为自变量，以下一个迭代坐标的函数值作为因变量的函数： \begin{equation} \phi(h)=f(x-h \nabla f(x)), \quad h \geq 0 \end{equation} 而且这个函数还被两个另外的函数夹住： \begin{equation} \phi_{1}(h)=f(x)-\alpha h|\nabla f(x)|^{2}, \quad \phi_{2}(h)=f(x)-\beta h|\nabla f(x)|^{2} \end{equation}</li> </ul> <h2 id="梯度下降的性能">梯度下降的性能</h2> <p>通过下面这个例子来衡量梯度下降的性能</p> <p>\begin{equation} \min _{x \in \mathbb{R}^{n}} f(x) \end{equation}</p> <p>其中 \(f \in C_{L}^{1,1}\left(\mathbb{R}^{n}\right)\)，并且我们假设这个函数在 \(\mathbb{R}^{n}\) 中是有下界的，也就是存在一个最小值。</p> <h3 id="下降步长和性能">下降步长和性能</h3> <p>如果我们考虑将 \(y=x-\nabla f(x)\) 作为梯度下降往前走的一个新的点，那么把 \(y=x-\nabla f(x)\) 带回到原始方程中就有：</p> <p>\begin{equation} \begin{aligned} f(y) &amp; \leq f(x)+\langle\nabla f(x), y-x\rangle+\frac{L}{2}|y-x|^{2} \\ &amp;=f(x)-h|\nabla f(x)|^{2}+\frac{h^{2}}{2} L|\nabla f(x)|^{2} \\ &amp;=f(x)-h\left(1-\frac{h}{2} L\right)|\nabla f(x)|^{2} \end{aligned} \end{equation}</p> <p>其中 \(h \in\left(0, \frac{2}{L}\right)\)。对于这个不等式，在Note 1 中的一阶逼近已经说明: \(f(y)=f(\overline{x})+\langle\nabla f(x), y-\overline{x}\rangle+ o(\\vert y-\overline{x}\\vert )\) 所以不等式中 \(\frac{L}{2}\\vert y-x\\vert ^{2}\) 显然大于高阶无穷小。</p> <p>所以我们就得到一个依赖于 \(h\) 的函数，要使在这一步迭代搜索的结果下降的最多（基于一种贪婪的原则），我们要让这个值趋向于最小，这样可以让这一步走到特别低的位置去。 \begin{equation} \Delta(h)=-h\left(1-\frac{h}{2} L\right) \rightarrow \min _{h} \end{equation} 所以计算它的导数，我们可以找到它的极值，也就是如果这个 \(h\) 是最优的，那它至少会先满足导数为零这个条件 \(\Delta^{\prime}(h)=h L-1=0\)，也就得到 \(h^{*}=\frac{1}{L}\) 当它的二阶导数满足 \(\Delta^{\prime \prime}(h)=L&gt;0\) 时可以确定这个值是一个让下降程度最大的极值点。而且这个 \(h\) 的最优取值和前面不等式 \(\frac{L}{2}\\vert y-x\\vert ^{2}\) 中的 \(L\) 的比例没有关系，只要使得不等式成立就行，最后它至少可以使原来要优化的函数下降以下这么多 \begin{equation} f(y) \leq f(x)-\frac{1}{2 L}|\nabla f(x)|^{2} \end{equation} \(L\) 是Lipschitz 常数，是这类要优化的函数预先满足的条件。</p> <p>下面分析前面讲过的三种梯度下降比例的性能：</p> <ul> <li> <strong>固定步长策略</strong>：令 \(x_{k+1}=x_{k}-h_{k} \nabla f\left(x_{k}\right)\) 有 \begin{equation} f\left(x_{k}\right)-f\left(x_{k+1}\right) \geq h\left(1-\frac{1}{2} L h\right)\left|\nabla f\left(x_{k}\right)\right|^{2} \end{equation} 可以推出 \(h_{k}=\frac{1}{L}\)</li> <li> <strong>完全松弛策略</strong>：完全松弛就是选择的最优值，所以也肯定不会比 \(h_{k}=\frac{1}{L}\) 差</li> <li> <p><strong>Goldsteim-Armijo规则</strong>：</p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>对于 $$\beta$$
</code></pre></div> </div> <p>\begin{equation} f\left(x_{k}\right)-f\left(x_{k+1}\right) \leq \beta\left\langle\nabla f\left(x_{k}\right), x_{k}-x_{k+1}\right\rangle=\beta h_{k}\left|\nabla f\left(x_{k}\right)\right|^{2} \end{equation} 对于 \(\alpha\) \begin{equation} f\left(x_{k}\right)-f\left(x_{k+1}\right) \geq \alpha\left\langle\nabla f\left(x_{k}\right), x_{k}-x_{k+1}\right\rangle=\alpha h_{k}\left|\nabla f\left(x_{k}\right)\right|^{2} \end{equation} 而基于前面下降量和 \(h\) 函数分析，我们有 \begin{equation} f\left(x_{k}\right)-f\left(x_{k+1}\right) \geq h_{k}\left(1-\frac{h_{k}}{2} L\right)\left|\nabla f\left(x_{k}\right)\right|^{2} \end{equation} 把这些得到的内容联立起来我们可以有 \begin{equation} f\left(x_{k}\right)-f\left(x_{k+1}\right) \geq \frac{2}{L} \alpha(1-\beta)\left|\nabla f\left(x_{k}\right)\right|^{2} \end{equation}</p> </li> </ul> <p>综上，这些步长的选取都是基于 \(L\) 的比例，而且在迭代过程中这种步长还都是固定的。我们定义这个比例为 \(\omega\) 把这些梯度下降法的相邻步骤的分析全部累计在一起可以得到 \begin{equation} \frac{\omega}{L} \sum_{k=0}^{N}\left|\nabla f\left(x_{k}\right)\right|^{2} \leq f\left(x_{0}\right)-f\left(x_{N+1}\right) \leq f\left(x_{0}\right)-f^{*} \end{equation}</p> <p>这个下降步骤过程中产生的全部中间值的和是有下界的，也就是这些所有值的和是不会比当前出发点和真正最优值的绝对差值更大的，所以这些迭代过程中计算的梯度数值（中间值）构成的序列是收敛的。 \begin{equation} \left|\nabla f\left(x_{k}\right)\right| \rightarrow 0 \quad \text { as } \quad k \rightarrow \infty \end{equation}</p> <h3 id="收敛速度">收敛速度</h3> <p>既然收敛，我们就可以对它的收敛速度（收敛率）进行评估，定义收敛速度（收敛率）为 \begin{equation} g_{N}^{<em>}=\min _{0 \leq k \leq N}\left|\nabla f\left(x_{k}\right)\right| \end{equation} 因为收敛序列的性质，它的最小值肯定是要小于等于序列的平均值，有下面的不等式 \begin{equation} g_{N}^{</em>} \leq \frac{1}{\sqrt{N+1}}\left[\frac{1}{\omega} L\left(f\left(x_{0}\right)-f^{*}\right)\right]^{1 / 2} \end{equation}</p> <p>考虑符合下面这一类的问题：</p> <p>模型</p> <ol> <li>无约束的最小值</li> <li> \[f \in C_{L}^{1,1}\left(\mathbb{R}^{n}\right)\] </li> <li>\(f(x)\) 有下界</li> </ol> <p>Oracle为一阶黑盒，而它的 \(\epsilon\) 解为 \(f(\overline{x}) \leq f\left(x_{0}\right),\left\\vert f^{\prime}(\overline{x})\right\\vert \leq \epsilon\)</p> <p>根据前面得到的梯度下降的收敛速度（收敛率）公式，可以知道如果要满足这个解的条件需要 \begin{equation} g_{N}^{<em>} \leq \frac{1}{\sqrt{N+1}}\left[\frac{1}{\omega} L\left(f\left(x_{0}\right)-f^{</em>}\right)\right]^{1 / 2} \leq \epsilon \end{equation} 也就是至多经过 \(N+1 \geq \frac{L}{\omega \epsilon^{2}}\left(f\left(x_{0}\right)-f^{*}\right)\) 能达到这种计算精度。这个作为复杂度上界已经很不错了，它不会随着 \(n\) 的变化而变化，不过现在这种方法的下界还不清楚。</p> <h3 id="局部收敛">局部收敛</h3> <p>以下的什么方式可以说这个梯度方法是局部收敛的，还是求最小值问题，首先使这类问题满足下面的假设</p> <ol> <li> \[f \in C_{M}^{2,2}\left(\mathbb{R}^{n}\right)\] </li> <li>函数 \(f\) 的局部最小化存在，它的 Hessian是正定的</li> <li>我们知道一些在 \(x^{*}\) 上关于 Hessian 的界 \(\mu I_{n} \leq \nabla^{2} f\left(x^{*}\right) \leq L I_{n}\)</li> <li>我们的初始点 \(x_0\) 和 \(x^{*}\) 足够近</li> </ol> <p>因为在最优点 \(x^{*}\) 函数的导数为0，所以 \begin{equation} \begin{aligned} \nabla f\left(x_{k}\right) &amp;=\nabla f\left(x_{k}\right)-\nabla f\left(x^{<em>}\right) \\ &amp;=\int_{0}^{1} \nabla^{2} f\left(x^{</em>}+\tau\left(x_{k}-x^{<em>}\right)\right)\left(x_{k}-x^{</em>}\right) d \tau \\ &amp;=G_{k}\left(x_{k}-x^{<em>}\right) \end{aligned} \end{equation} 其中 \(G*{k}=\int*{0}^{1} \nabla^{2} f\left(x^{_}+\tau\left(x*{k}-x^{*}\right)\right) d \tau\) 二阶导数经过这个步骤积分完之后也就得到 \(\nabla f(x_k)/(x_k-x^{*})\) 因为根据梯度下降，从 \(x_k\) 到 \(x*{k+1}\) 走过的位移为 \(-h_k \nabla f(x_k)\) 也就得到 \begin{equation} x_{k+1}-x^{</em>}=x_{k}-x^{<em>}-h_{k} G_{k}\left(x_{k}-x^{</em>}\right)=\left(I_{n}-h_{k} G_{k}\right)\left(x_{k}-x^{_}\right) \end{equation} 有一种叫做收缩映射（contracting mappings）<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>的标准技术可以分析这个过程，令序列 \(\left\{a_{k}\right\}\) 的定义如下： \begin{equation} a_{0} \in \mathbb{R}^{n}, a_{k+1}=A_{k} a_{k} \end{equation} 这里的 \(A_k\) 是一个 \((n\times n)\) 的矩阵，且满足 \(\left\\vert A_{k}\right\\vert \leq 1-q\) 其中 \(q\in (0,1)\)，这个序列的收敛速度（收敛率）是趋于0的。</p> <p>\begin{equation} \left|a_{k+1}\right| \leq(1-q)\left|a_{k}\right| \leq(1-q)^{k+1}\left|a_{0}\right| \rightarrow 0 \end{equation} 因为在上面的例子中，我们需要估计 \(\left\\vert I_{n}-h_{k} G_{k}\right\\vert\) 所以沿用第一节中几何解释的方法，选择一个距离，记 \(r_{k}=\left\\vert x_{k}-x^{*}\right\\vert\)，根据几何解释的推论 \(\left\\vert \nabla^{2} f(x)-\nabla^{2} f(y)\right\\vert \leq M\\vert x-y\\vert , \quad \forall x, y \in \mathbb{R}^{n}\) 根据 \(\nabla^2 f(x^{*})\) 上下界得到 \begin{equation} \nabla^{2} f\left(x^{<em>}\right)-\tau M r_{k} I_{n} \preceq \nabla^{2} f\left(x^{</em>}+\tau\left(x_{k}-x^{<em>}\right)\right) \preceq \nabla^{2} f\left(x^{</em>}\right)+\tau M r_{k} I_{n} \end{equation} 对 \(\tau\) 从 \(0\sim 1\) 积分也就可以再写成下面的形式，因为 \(\mu\) 和 \(L\) 是我们已知的 \(f(x^{*})\) 的信息 \begin{equation} \left(\mu-\frac{r_{k}}{2} M\right) I_{n} \preceq G_{k} \preceq\left(L+\frac{r_{k}}{2} M\right) I_{n} \end{equation} 将上面的这个不等式再配出 \(\left\\vert I_{n}-h_{k} G_{k}\right\\vert\) 的形式则有以下形式 \begin{equation} \left(1-h_{k}\left(L+\frac{r_{k}}{2} M\right)\right) I_{n} \leq I_{n}-h_{k} G_{k} \preceq\left(1-h_{k}\left(\mu-\frac{r_{k}}{2} M\right)\right) I_{n} \end{equation} 为了表述方便将不等式两边分别表示为 \(a_{k}(h)=1-h\left(\mu-\frac{r_{k}}{2} M\right)\) 和 \(b_{k}(h)=h\left(L+\frac{r_{k}}{2} M\right)-1\)，重写成下面的形式 \begin{equation} \left|I_{n}-h_{k} G_{k}\right| \leq \max \left{a_{k}\left(h_{k}\right), b_{k}\left(h_{k}\right)\right} \end{equation} 如果 \(0 &lt; r_{k} &lt;\overline{r} \equiv \frac{2 \mu}{M}\) 那么 \(a_k(\cdot)\) 就是严格下降的函数，并且可以保证 \(\left\\vert I_{n}-h_{k} G_{k}\right\\vert &lt;1\) 只要 \(h_k\) 足够小，就能够保证 \(r_{k+1} &lt; r_k\)。有多个梯度选择的方法可以使用，例如可以选择已经证明的 \(h_k=\frac{1}{L}\) 优化这个最小化最大值的思路就是让 \begin{equation} \max \left{a_{k}(h), b_{k}(h)\right} \rightarrow \min _{h} \end{equation} 假设 \(r_0 &lt; \bar{r}\) 那么按照最优的方式构造的序列就可以保证 \(r_{k+1} &lt; r_{k}&lt;\overline{r}\)，从而找到最好的步长 \(h^{*}_{k}\) \begin{equation} a_{k}(h)=b_{k}(h) \quad \Leftrightarrow \quad 1-h\left(\mu-\frac{r_{k}}{2} M\right)=h\left(L+\frac{r_{k}}{2} M\right)-1 \end{equation} 最终得到这个最佳的步长是不依赖于 \(M\) 的值 \begin{equation} h_{k}^{_}=\frac{2}{L+\mu} \end{equation} 把这个最优值带回到原来的不等式中得到 \(r_{k+1}\) 和 \(r_k\) 之间的关系 \begin{equation} r_{k+1} \leq \frac{(L-\mu) r_{k}}{L+\mu}+\frac{M r_{k}^{2}}{L+\mu} \end{equation} 然后根据这个序列的关系来估计整个过程中的收敛速度，令 \(q=\frac{2 \mu}{L+\mu}\) 根据 \(h^{_}\) 带入解的形式的公式得到 \(a_k=\frac{M}{L+\mu} r_{k}( &lt; q)\) 所以带回到原来的不等式关系中去再配成数列通项形式 \begin{equation} a_{k+1} \leq(1-q) a_{k}+a_{k}^{2}=a_{k}\left(1+\left(a_{k}-q\right)\right)=\frac{a_{k}\left(1-\left(a_{k}-q\right)^{2}\right)}{1-\left(a_{k}-q\right)} \leq \frac{a_{k}}{1+q-a_{k}} \end{equation} 所以取不等式最左边和最右边，取倒数可以得到 \begin{equation} \frac{q}{a_{k+1}}-1 \geq \frac{q(1+q)}{a_{k}}-q-1=(1+q)\left(\frac{q}{a_{k}}-1\right) \end{equation} 我们得到等比数列的通项形式之后可以再进一步知道它和第一项之间关系 \begin{equation} \begin{aligned} \frac{q}{a_{k}}-1 &amp; \geq(1+q)^{k}\left(\frac{q}{a_{0}}-1\right)=(1+q)^{k}\left(\frac{2 \mu}{L+\mu} \cdot \frac{L+\mu}{r_{0} M}-1\right) \\ &amp;=(1+q)^{k}\left(\frac{\overline{r}}{r_{0}}-1\right) \end{aligned} \end{equation} 因此 \begin{equation} a_{k} \leq \frac{q r_{0}}{r_{0}+(1+q)^{k}\left(\overline{r}-r_{0}\right)} \leq \frac{q r_{0}}{\overline{r}-r_{0}}\left(\frac{1}{1+q}\right)^{k} \end{equation} 证明了整个过程中的收敛率。</p> <blockquote> <p>如果有满足我们以上假设的函数 \(f(\cdot)\) 并且让它的起始点 \(x_0\) 和最优的局部最低点 \(x^{*}\) 已经非常靠近了，那么有：</p> \[r_{0}=\left\|x_{0}-x^{*}\right\|&lt;\overline{r}=\frac{2 \mu}{M}\] <p>带入前面计算得到的最优的步长 \(h_{k}^{*}=\frac{2}{L+\mu}\) 可以计算得到收敛到如下： \begin{equation} \left|x_{k}-x^{*}\right| \leq \frac{\overline{r} r_{0}}{\overline{r}-r_{0}}\left(1-\frac{2 \mu}{L+3 \mu}\right)^{k} \end{equation} 这种就称为线性收敛率。</p> </blockquote> <hr> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>NESTEROV J E. Lectures on convex optimization[M]. 2018. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:2" role="doc-endnote"> <p><a href="https://en.wikipedia.org/wiki/Contraction_mapping" rel="external nofollow noopener" target="_blank">Contraction mapping Wiki</a> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">↩</a></p> </li> </ol> </div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2019/Convex-Optimization-Note-1/">Convex Optimization Note 1 | Introduction</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2019/Online-Optimization/">Online Optimization</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/optimal-transport/">最优传输理论</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/Poincare-Ball-Model/">庞加莱球模型</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Extend-LLMs-Context-Window/">Extend LLMs Context Window</a> </li> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"lccurious/lccurious.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Zenan Huang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>