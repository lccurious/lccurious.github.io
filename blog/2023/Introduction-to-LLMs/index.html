<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Introduction to LLMs | Curiousity Hub </title> <meta name="author" content="Zenan Huang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="causal-learning, causal-discovery, transfer-learning, LLMs, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon-32x32-next-my.png?b29ea1d332f6bc3c021e15d78d266303"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://lccurious.github.io/blog/2023/Introduction-to-LLMs/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Curiousity Hub </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/cv/">cv</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <style type="text/css">.row p{mjx-container[jax="CHTML"][display="true"]{display:unset}}.caption{mjx-container[jax="CHTML"][display="true"]{display:unset}}</style> <div class="post"> <header class="post-header"> <h1 class="post-title">Introduction to LLMs</h1> <p class="post-meta"> December 14, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/convex-optimization"> <i class="fa-solid fa-hashtag fa-sm"></i> convex optimization</a>   <a href="/blog/tag/pattern-recognition"> <i class="fa-solid fa-hashtag fa-sm"></i> pattern recognition</a>     ·   <a href="/blog/category/machine-learning"> <i class="fa-solid fa-tag fa-sm"></i> Machine Learning</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>准确地说这里是指 GPT，OpenAI 团队也发布了一篇简短的文章用于解释这个生成式预训练 Transformer<sup id="fnref:gpt" role="doc-noteref"><a href="#fn:gpt" class="footnote" rel="footnote">1</a></sup>。</p> <div class="row"> <div class="col-lg-10 mt-3 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2023-12-14-Introduction-to-LLMs/GPTs-tree-480.webp 480w,/assets/img/2023-12-14-Introduction-to-LLMs/GPTs-tree-800.webp 800w,/assets/img/2023-12-14-Introduction-to-LLMs/GPTs-tree-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2023-12-14-Introduction-to-LLMs/GPTs-tree.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="GPTs Tree" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">GPTs Tree</figcaption> </figure> </div> </div> <h2 id="brief-introduction-to-gpt">Brief Introduction to GPT</h2> <p>主要的设计和创作者是OpenAI，但是更进一步的实现和研究其实来自于非常多的其他团队，包括硬件部分，分布式训练，RLHF等。在 2023 年 3 月份发布的 GPT-4 技术报告 <a href="http://arxiv.org/abs/2303.08774" rel="external nofollow noopener" target="_blank">OpenAI, 2023</a> 也已经比较详尽地介绍了关于模型的能力范围细节。</p> <h3 id="模型架构">模型架构</h3> <p>在 Attention is All You Need <a href="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" rel="external nofollow noopener" target="_blank">Vaswani et al., 2017</a> 原文中提出的 Transformer 架构包含编码器和解码器用于序列任务的编码。在后来的 Generative Pre-trained Transformer 的开发中，OpenAI 选择了 Decoder-Only 的思路，Google 先选择了 Encoder-Decoder 的架构。OpenAI 的研究人员表示在 Decoder-Only 的架构设计下，模型更难通过模仿数据中的表面规律来产生似是而非的回答。这提高了模型最后的学习难度，只有模型学到更加本质的规律以后才能使得其输出看起来像那么回事。</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2023-12-14-Introduction-to-LLMs/Transformers-decoder-only-480.webp 480w,/assets/img/2023-12-14-Introduction-to-LLMs/Transformers-decoder-only-800.webp 800w,/assets/img/2023-12-14-Introduction-to-LLMs/Transformers-decoder-only-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2023-12-14-Introduction-to-LLMs/Transformers-decoder-only.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Comparsion of Transformers and Decoder-only" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Comparsion of Transformers and Decoder-only</figcaption> </figure> </div> </div> <p>所以后来除了 OpenAI 团队以外，很多其他团队也发现了随着模型规模的增加，Decoder-Only 的架构确实能够非常好地完成任务<sup id="fnref:zhihu" role="doc-noteref"><a href="#fn:zhihu" class="footnote" rel="footnote">2</a></sup>，也有着更高的上限和多样性。 目前的主流 GPT 系列是以 Transformer <a href="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" rel="external nofollow noopener" target="_blank">Vaswani et al., 2017</a> 作为基本模块进行组合实现的。尽管它们都遵从类似的架构形式，但是也还是有可能使用其他架构的模块进行组装。例如 RetNet <a href="http://arxiv.org/abs/2307.08621" rel="external nofollow noopener" target="_blank">Sun et al., 2023</a> 提出针对 Transformer 的三种改进，降低模型的计算开销并提升推理阶段的并行性能。RWKV <a href="http://arxiv.org/abs/2305.13048" rel="external nofollow noopener" target="_blank">Peng et al., 2023</a> 也是一种针对 Transformer 的改进。</p> <h3 id="关键模块">关键模块</h3> <div class="row"> <div class="col-lg-8 mt-3 mt-md-0 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2023-12-14-Introduction-to-LLMs/GPTs-modules-480.webp 480w,/assets/img/2023-12-14-Introduction-to-LLMs/GPTs-modules-800.webp 800w,/assets/img/2023-12-14-Introduction-to-LLMs/GPTs-modules-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2023-12-14-Introduction-to-LLMs/GPTs-modules.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="GPTs modules" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">GPTs modules</figcaption> </figure> </div> </div> <p>从 LLaMA 开始的后续开源 GPT 系列中，都用了一些类似的内部架构设计。用 RoPE 旋转位置编码<a href="http://arxiv.org/abs/2104.09864" rel="external nofollow noopener" target="_blank">Su et al., 2023</a> 来代替基础版本的 Transformer 中的绝对位置编码，好处是可以更加高效地表示不同编码位置之间的相对距离。</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2023-12-14-Introduction-to-LLMs/RoPE-480.webp 480w,/assets/img/2023-12-14-Introduction-to-LLMs/RoPE-800.webp 800w,/assets/img/2023-12-14-Introduction-to-LLMs/RoPE-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2023-12-14-Introduction-to-LLMs/RoPE.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="RoPE" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">RoPE</figcaption> </figure> </div> </div> <p>\begin{equation} f_{q,k}(\boldsymbol{x}_{m},m)=\boldsymbol{R}^{d}_{\Theta,m}\boldsymbol{W}_{{q,k}}\boldsymbol{x}_{m} \end{equation}</p> <p>其中 \(\boldsymbol{R}\) 的具体形式为：</p> <p>\begin{equation} \boldsymbol{R}^d_{\Theta,m} = \begin{pmatrix} \cos{m\theta_1}&amp; -\sin{m\theta_1}&amp;0&amp;0&amp;\cdots&amp;0&amp;0\\ \sin{m\theta_1}&amp;\cos{m\theta_1}&amp;0&amp;0&amp;\cdots&amp;0&amp;0 \\ 0&amp;0&amp;\cos{m\theta_2}&amp; -\sin{m\theta_2}&amp;\cdots&amp;0&amp;0\\ 0&amp;0&amp;\sin{m\theta_2}&amp;\cos{m\theta_2}&amp;\cdots&amp;0&amp;0 \\ \vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots&amp;\vdots\\ 0&amp;0&amp;0&amp;0&amp;\cdots&amp;\cos{m\theta_{d/2}}&amp; -\sin{m\theta_{d/2}}\\ 0&amp;0&amp;0&amp;0&amp;\cdots&amp;\sin{m\theta_{d/2}}&amp;\cos{m\theta_{d/2}} \end{pmatrix} \end{equation}</p> <p>通过这种形式的位置编码后，计算自注意力机制时就可以推导为如下形式：</p> <p>\begin{equation} \boldsymbol{q}^{\top}_{m}\boldsymbol{k}_{n}=(\boldsymbol{R}^{d}_{\Theta,m}\boldsymbol{W}_{q}\boldsymbol{x}_{m})^{\top}(\boldsymbol{R}^{d}_{\Theta,n}\boldsymbol{W}_{k}\boldsymbol{x}_{n})=\boldsymbol{x}^{\top}\boldsymbol{W}_{q}\boldsymbol{R}^{d}_{\Theta,n-m}\boldsymbol{W}_{k}\boldsymbol{x}_{n} \end{equation}</p> <p>在 LLaMA2 中的一个关键性改进则在于引入了 Grouped-query 技术 <a href="http://arxiv.org/abs/2305.13245" rel="external nofollow noopener" target="_blank">Ainslie et al., 2023</a> ：</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2023-12-14-Introduction-to-LLMs/group_query-480.webp 480w,/assets/img/2023-12-14-Introduction-to-LLMs/group_query-800.webp 800w,/assets/img/2023-12-14-Introduction-to-LLMs/group_query-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2023-12-14-Introduction-to-LLMs/group_query.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Group query" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Group query</figcaption> </figure> </div> </div> <h3 id="mistral-7b-架构">Mistral-7B 架构</h3> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2023-12-14-Introduction-to-LLMs/Mistral-7B-480.webp 480w,/assets/img/2023-12-14-Introduction-to-LLMs/Mistral-7B-800.webp 800w,/assets/img/2023-12-14-Introduction-to-LLMs/Mistral-7B-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2023-12-14-Introduction-to-LLMs/Mistral-7B.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Mistral 7B" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Mistral 7B</figcaption> </figure> </div> </div> <p>在 Transformers 模型的序列建模中需要引入一个因果掩码机制，在建模后继序列内容的时候保证仅引入之前的序列信息而不带来信息的泄露。掩码矩阵通常为一个上三角矩阵，Mistral-7B 模型在设计过程中改了这一步，一个后继 token 的建模只能看到离当前位置较近的一些前序 tokens，这样就可以在很大程度上减少整体模型的总体计算量。尽管在当前层每个 token 只能看到部分前继的信息，但是网络的层数越靠后，前面的信息就像生物富集似的全部汇聚在最后，所以整体效果上每个 token 的建模过程中还是用了它的全部前序信息。</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2023-12-14-Introduction-to-LLMs/mistral-7b-performance-480.webp 480w,/assets/img/2023-12-14-Introduction-to-LLMs/mistral-7b-performance-800.webp 800w,/assets/img/2023-12-14-Introduction-to-LLMs/mistral-7b-performance-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2023-12-14-Introduction-to-LLMs/mistral-7b-performance.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Mistral-7B performance" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Mistral-7B performance</figcaption> </figure> </div> </div> <p>当然，Mistral-7B 的性能测试表示也非常亮眼。</p> <h2 id="总体训练流程">总体训练流程</h2> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2023-12-14-Introduction-to-LLMs/LLMs_training-480.webp 480w,/assets/img/2023-12-14-Introduction-to-LLMs/LLMs_training-800.webp 800w,/assets/img/2023-12-14-Introduction-to-LLMs/LLMs_training-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2023-12-14-Introduction-to-LLMs/LLMs_training.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="LLMs training" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">LLMs training</figcaption> </figure> </div> </div> <p>根据 LLaMA 和 OpenAI 的公开内容，模型总体训练分为三个阶段：</p> <ol> <li>通过 Next Token Prediction 实现模型在海量互联网数据上的预训练</li> <li>将预训练完成的模型采用有监督微调策略来实现特定任务上的改进</li> <li>引入基于人类反馈的强化学习策略使模型进一步理解各类人类任务</li> </ol> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2023-12-14-Introduction-to-LLMs/RLHF-480.webp 480w,/assets/img/2023-12-14-Introduction-to-LLMs/RLHF-800.webp 800w,/assets/img/2023-12-14-Introduction-to-LLMs/RLHF-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2023-12-14-Introduction-to-LLMs/RLHF.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="RLHF" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">RLHF</figcaption> </figure> </div> </div> <h2 id="compression--intelligence">Compression &amp; Intelligence</h2> <p>DeepMind 的研究表示模型即使没有针对压缩任务进行特别设计和优化，也能够作为一种通用压缩器。<a href="http://arxiv.org/abs/2309.10668" rel="external nofollow noopener" target="_blank">Delétang et al., 2023</a> 中表示经过预训练后的模型能够在图像、语音、文本这些类型的数据压缩能力上都不亚于专业的 gzip 等压缩软件。</p> <blockquote> <p>We empirically demonstrate that these models, while (meta-)trained primarily on text, also achieve state-of-the-art compression rates across different data modalities, using their context to condition a general-purpose compressor to excel at a particular task.</p> </blockquote> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2023-12-14-Introduction-to-LLMs/arithmetic_compression-480.webp 480w,/assets/img/2023-12-14-Introduction-to-LLMs/arithmetic_compression-800.webp 800w,/assets/img/2023-12-14-Introduction-to-LLMs/arithmetic_compression-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2023-12-14-Introduction-to-LLMs/arithmetic_compression.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="arithmetic compression" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">arithmetic compression</figcaption> </figure> </div> </div> <p>以一串字符序列为例，字符串中的每个字母都能对应于一个边缘概率，表示它在环境中出现的概率。</p> <ol> <li>第一个字符为 A，编码应该落到 \([0, 0.45]\) 这个区间内</li> <li>下一个字符为 I，细化一下区间，编码应该落在 \([0.09, 0.36]\) 这个区间内</li> <li>下一个字符为 X，细化一下区间，编码应该落在 \([0.266, 0.36]\) 这个区间内</li> <li>最后一个字符为 I，细化一下区间，编码应该落在 \([0.322, 0.341]\) 这个区间内</li> <li>在 \([0.322, 0.341]\) 这个区间内任意取一个小数即可，因为这个编码肯定是小数，所以只要用01编码小数点后的数字即可，图中示意的编码即为 <code class="language-plaintext highlighter-rouge">b0101010</code> </li> </ol> <div class="row"> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2023-12-14-Introduction-to-LLMs/arithmetic_algorithm-480.webp 480w,/assets/img/2023-12-14-Introduction-to-LLMs/arithmetic_algorithm-800.webp 800w,/assets/img/2023-12-14-Introduction-to-LLMs/arithmetic_algorithm-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2023-12-14-Introduction-to-LLMs/arithmetic_algorithm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Wiki arithmetic algorithm" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Wiki arithmetic algorithm</figcaption> </figure> </div> <div class="col-sm-8 mt-3 mt-md-0"> <p>算数编码的原理与哈夫曼编码类似，一个长串最终会被映射到某个01字符串上，算数编码的理论极限更加接近于香农熵的极限：</p> <p>\begin{equation} H=-\sum^{N}_{i=1}P(i)\log_{b}P(i) \end{equation}</p> <p>如果 $$b=2$$，熵的单位为bit，如果 $$b=e$$，熵的单位为 奈特。</p> </div> </div> <p>在算数编码的流程开始时，需要所有基本字符对应的边缘概率，这个边缘概率反应了每个字符在这个场景中的统计概率。对于纯粹的文件压缩任务而言，这可以通过先读一遍文件所有的信息来获取。但是对于通用压缩的模型而言，它得掌握全宇宙中每个字符对应的基本出现概率。这显然无法做到，所以只能通过近似的方式来做。这个提升这个近似的过程其实就与我们的模型训练过程对应：</p> <p>\begin{equation} H(\rho, \hat{\rho}):=\mathbb{E}_{x\sim \rho}\left[\sum^{n}_{i=1}-\log_{2}\hat{\rho}(x_{i}|x_{&lt;i})\right]. \end{equation}</p> <p>这个极大似然的目标也就对应于最小化序列 \(x_{1:n}\) 压缩编码后的编码长度 \(-\sum^{n}_{i=1}\log_{2}\hat{\rho}(x_{i}\vert x_{&lt;i})\)，因为完美的编码结果就是最小的，所以训练过程就是最小化参数降低模型的编码长度。这个损失项也就完整对应于当前各类大模型的预训练目标函数。</p> <p>相应的，如果把这个编码形式改一改，刚好对应于：</p> <p>\begin{equation} \rho(x_{i}|x_{&lt;i})=2^{\ell_{c}(x_{&lt;i})-\ell_{c}([x_{&lt;i},x_{i}])} \end{equation}</p> <p>这就是 Next-Token prediction 的推理形式。</p> <h2 id="实验设计">实验设计</h2> <p>大语言模型的输入上下文有窗口极限限制，一般的数据不能被一口气全部塞进模型，为了解决此类问题就引出两类策略：</p> <ol> <li>使用滑动编码策略，每个压缩器都在数据上滑动，每编码一个新的 byte 都会用到前 \(C-1\) 个 bytes</li> <li>直接将输入分解为多个长度相同的块，然后一一将这些块输入到编码器中进行编码</li> </ol> <p>在实际的实验中，本文的实验采用了平均分快的方法，所有的输入数据集被分割为多个 2048 bytes 的 chunk，然后这些输入块将被一个一个输入压缩器。</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2023-12-14-Introduction-to-LLMs/compression_results-480.webp 480w,/assets/img/2023-12-14-Introduction-to-LLMs/compression_results-800.webp 800w,/assets/img/2023-12-14-Introduction-to-LLMs/compression_results-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2023-12-14-Introduction-to-LLMs/compression_results.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Compression results" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Compression results</figcaption> </figure> </div> </div> <p>从实验结果可以看到各类大模型具备了比肩专业的压缩编码软件的能力。</p> <ul> <li> <p>更多参考解释：</p> <blockquote> <p><a href="https://zhuanlan.zhihu.com/p/657899967" rel="external nofollow noopener" target="_blank">Language Modeling Is Compression?Google DeepMind 文章与Open AI的两次压缩与智能/泛化talk的笔记和想法 - beiluo的文章 - 知乎</a></p> <p><a href="https://zhuanlan.zhihu.com/p/619511222" rel="external nofollow noopener" target="_blank">压缩下一个 token 通向超过人类的智能 - 周昕宇的文章 - 知乎</a></p> </blockquote> </li> </ul> <p>所以本质上，任何其他可以建模为极大似然估计的任务，都可以直接作为大模型的 next-token-prediction 任务。</p> <p>是的，您的理解是正确的。在使用大型语言模型（如参数为 \(\theta\) 的模型）进行编码和解码的过程中，模型的作用是估计给定前文的条件下，每个字符的概率分布。这一过程可以分为以下几个步骤：</p> <h3 id="编码过程">编码过程</h3> <ol> <li> <strong>概率分布估计</strong>：对于序列中的每个字符 \(s_n\)，使用模型 \(p_{\theta}\) 来计算在给定前面字符 \(s_{&lt;n}\) 的条件下，每个可能字符的概率 \(p_{\theta}(s_n\vert s_{&lt;n})\)。</li> <li> <strong>转换为二进制编码</strong>：基于这些概率估计，使用类似于算术编码的方法将整个序列转换为一个二进制编码。这个过程涉及根据每个字符的概率逐步细化一个表示整个序列的数值区间。</li> </ol> <h3 id="解码过程">解码过程</h3> <ol> <li> <strong>读取编码数据</strong>：解码过程从读取代表原始序列的二进制编码开始。</li> <li> <strong>迭代概率计算</strong>：使用模型 \(p_{\theta}\) 逐步重建序列。对于每个位置 \(n\)，模型计算 \(p_{\theta}(s_0)\)， \(p_{\theta}(s_1\vert s_0)\)，…， \(p_{\theta}(s_n\vert s_{&lt;n})\)。</li> <li> <strong>区间判断</strong>：根据编码数据和这些概率估计，确定每个字符落在哪个概率区间内。这允许模型逐步确定并恢复原始序列中的每个字符。</li> </ol> <p>这个过程依赖于模型对字符序列概率分布的准确估计。理论上，如果模型的估计非常准确，这种方法可以非常有效地压缩和恢复数据。然而，实际应用中的效率和准确性可能会受到多种因素的影响，包括模型的复杂性、训练数据的质量和范围，以及实际实现时的各种优化和权衡。</p> <h2 id="chinchilla-rule">Chinchilla Rule</h2> <p>\begin{equation} \mathrm{FLOPs}(N,D)=C:N_{opt}(C),D_{opt}(C)=\mathop{\rm argmin}_{N,D~s.t.~\mathrm{FLOPs}(N,D)=C}L(N,D). \end{equation}</p> <p>根据 Chinachilla Rule 可以估算出在给定计算量规模的前提下，如何设置参数量和训练数据集规模来实现最大化的模型优化水准。 所以经过一系列的工程实验之后，研究者们拟合出一个经验曲线：</p> <p>\begin{equation} \hat{L}(N,D)=E+\frac{A}{N^{\alpha}}+\frac{B}{D^{\beta}} \end{equation}</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2023-12-14-Introduction-to-LLMs/Chinachilla_curve-480.webp 480w,/assets/img/2023-12-14-Introduction-to-LLMs/Chinachilla_curve-800.webp 800w,/assets/img/2023-12-14-Introduction-to-LLMs/Chinachilla_curve-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2023-12-14-Introduction-to-LLMs/Chinachilla_curve.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Chinachilla curves" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Chinachilla curves</figcaption> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2023-12-14-Introduction-to-LLMs/Chinachilla_fitness-480.webp 480w,/assets/img/2023-12-14-Introduction-to-LLMs/Chinachilla_fitness-800.webp 800w,/assets/img/2023-12-14-Introduction-to-LLMs/Chinachilla_fitness-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2023-12-14-Introduction-to-LLMs/Chinachilla_fitness.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Chinachilla fitness" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Chinachilla fitness</figcaption> </figure> </div> </div> <p>但是后面又有一些讨论表示，Chinachilla Rule 是因为实验还是没有做到训练时间足够长。以及后面出现的 Mistral-7B 模型似乎也反驳了这类问题。</p> <h2 id="模型评估">模型评估</h2> <p>当评估一个经过训练和调整的模型时，可以从多个方面进行考量，包括知识与能力、伦理与安全以及垂直领域评估。以下是一些常见的评估指标及其简要说明：</p> <ul> <li> <strong>Perplexity (困惑度)</strong>：这是一个衡量语言模型性能的标准指标，用于评估模型对测试数据的预测能力。困惑度越低，表明模型对语言的预测越准确。</li> <li> <strong>BLEU (双语互译质量评估)</strong>：主要用于机器翻译，通过比较机器翻译输出与一系列参考翻译之间的重叠度来评估翻译质量。</li> <li> <strong>ROUGE (答案生成评估指标)</strong>：常用于评估自动摘要或机器翻译的质量，主要通过比较生成的摘要或翻译与参考摘要或翻译之间的重叠来进行评估。</li> <li> <strong>METEOR (翻译结果评估)</strong>：这是另一种用于评估机器翻译质量的指标，它不仅考虑词汇的精确匹配，还考虑了同义词和词形变化，通常认为比BLEU更精细。</li> <li> <strong>Human Evaluation (人工评估)</strong>：这涉及到人类评审员对模型的输出进行评估，通常考虑准确性、可读性和相关性等多个维度。人工评估可以提供更直观、更全面的评价。</li> <li> <strong>Zero-shot Evaluation (零样本评估)</strong>：这是一种测试模型在没有接受特定任务训练的情况下对新任务的处理能力的方法。零样本评估检验模型的泛化能力和对新领域、新类型数据的适应性。</li> </ul> <p>此外还有一些比较典型的评估指标。</p> <hr> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:gpt" role="doc-endnote"> <p><a href="https://openai.com/research/language-unsupervised" rel="external nofollow noopener" target="_blank">Improving language understanding with unsupervised learning</a> <a href="#fnref:gpt" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:zhihu" role="doc-endnote"> <p><a href="https://www.zhihu.com/question/588325646" rel="external nofollow noopener" target="_blank">为什么现在的LLM都是Decoder only的架构？ - 知乎</a> <a href="#fnref:zhihu" class="reversefootnote" role="doc-backlink">↩</a></p> </li> </ol> </div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/Word-Tricks/">Word 排版技巧</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2018/Black-First-Won/">Black First Won</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2017/%E9%92%B1%E7%A9%86%E5%85%88%E7%94%9F/">钱穆先生</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2015/images/">a post with images</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/Poincare-Ball-Model/">庞加莱球模型</a> </li> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"lccurious/lccurious.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Zenan Huang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>